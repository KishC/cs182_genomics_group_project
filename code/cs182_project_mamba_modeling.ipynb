{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>coord</th>\n",
       "      <th>kind</th>\n",
       "      <th>transcript</th>\n",
       "      <th>strand</th>\n",
       "      <th>chrom</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>sequence</th>\n",
       "      <th>win_start</th>\n",
       "      <th>win_end</th>\n",
       "      <th>is_truncated</th>\n",
       "      <th>motif_len</th>\n",
       "      <th>motif</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NC_050096.1_35318_donor</td>\n",
       "      <td>35318</td>\n",
       "      <td>0</td>\n",
       "      <td>XM_020544715.3</td>\n",
       "      <td>+</td>\n",
       "      <td>NC_050096.1</td>\n",
       "      <td>35319</td>\n",
       "      <td>35320</td>\n",
       "      <td>GGGCCCGGCTGGGCCTCAGCGGGGTCGTCGAGATGGAGATGGGGAG...</td>\n",
       "      <td>35118</td>\n",
       "      <td>35520</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>GT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NC_050096.1_34607_acceptor</td>\n",
       "      <td>34607</td>\n",
       "      <td>1</td>\n",
       "      <td>XM_020544715.3</td>\n",
       "      <td>+</td>\n",
       "      <td>NC_050096.1</td>\n",
       "      <td>34605</td>\n",
       "      <td>34606</td>\n",
       "      <td>TCCGGTGATTAATTTGTCCTTATACCTTTACAACAAAAATTCACTA...</td>\n",
       "      <td>34404</td>\n",
       "      <td>34806</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>TG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NC_050096.1_36174_donor</td>\n",
       "      <td>36174</td>\n",
       "      <td>0</td>\n",
       "      <td>XM_020544715.3</td>\n",
       "      <td>+</td>\n",
       "      <td>NC_050096.1</td>\n",
       "      <td>36175</td>\n",
       "      <td>36176</td>\n",
       "      <td>ATAATATGTTCATTATATCACAACACTCTTTTCTTATGGAGTCGTG...</td>\n",
       "      <td>35974</td>\n",
       "      <td>36376</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>GT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NC_050096.1_36037_acceptor</td>\n",
       "      <td>36037</td>\n",
       "      <td>1</td>\n",
       "      <td>XM_020544715.3</td>\n",
       "      <td>+</td>\n",
       "      <td>NC_050096.1</td>\n",
       "      <td>36035</td>\n",
       "      <td>36036</td>\n",
       "      <td>GCACAAAACTAACTAAAGGAATCATTCTGATAGATAACACTATAAA...</td>\n",
       "      <td>35834</td>\n",
       "      <td>36236</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>AG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NC_050096.1_36504_donor</td>\n",
       "      <td>36504</td>\n",
       "      <td>0</td>\n",
       "      <td>XM_020544715.3</td>\n",
       "      <td>+</td>\n",
       "      <td>NC_050096.1</td>\n",
       "      <td>36505</td>\n",
       "      <td>36506</td>\n",
       "      <td>TGTCATTTCCTTACCTCATTGAATCATTTCCGATGCTTCTTCTCTG...</td>\n",
       "      <td>36304</td>\n",
       "      <td>36706</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>GT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           id  coord  kind      transcript strand  \\\n",
       "0     NC_050096.1_35318_donor  35318     0  XM_020544715.3      +   \n",
       "1  NC_050096.1_34607_acceptor  34607     1  XM_020544715.3      +   \n",
       "2     NC_050096.1_36174_donor  36174     0  XM_020544715.3      +   \n",
       "3  NC_050096.1_36037_acceptor  36037     1  XM_020544715.3      +   \n",
       "4     NC_050096.1_36504_donor  36504     0  XM_020544715.3      +   \n",
       "\n",
       "         chrom  start    end  \\\n",
       "0  NC_050096.1  35319  35320   \n",
       "1  NC_050096.1  34605  34606   \n",
       "2  NC_050096.1  36175  36176   \n",
       "3  NC_050096.1  36035  36036   \n",
       "4  NC_050096.1  36505  36506   \n",
       "\n",
       "                                            sequence  win_start  win_end  \\\n",
       "0  GGGCCCGGCTGGGCCTCAGCGGGGTCGTCGAGATGGAGATGGGGAG...      35118    35520   \n",
       "1  TCCGGTGATTAATTTGTCCTTATACCTTTACAACAAAAATTCACTA...      34404    34806   \n",
       "2  ATAATATGTTCATTATATCACAACACTCTTTTCTTATGGAGTCGTG...      35974    36376   \n",
       "3  GCACAAAACTAACTAAAGGAATCATTCTGATAGATAACACTATAAA...      35834    36236   \n",
       "4  TGTCATTTCCTTACCTCATTGAATCATTTCCGATGCTTCTTCTCTG...      36304    36706   \n",
       "\n",
       "   is_truncated  motif_len motif  \n",
       "0         False          2    GT  \n",
       "1         False          2    TG  \n",
       "2         False          2    GT  \n",
       "3         False          2    AG  \n",
       "4         False          2    GT  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splice_df = pd.read_csv('splice_sites_full_centered_balanced_correct_V3.csv')\n",
    "splice_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data with stratification\n",
    "train_df, temp_df = train_test_split(splice_df, test_size=0.3, stratify=splice_df['kind'], random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['kind'], random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "/nas/ucb/kishorechidambaram/anaconda3/envs/mamba_env/lib/python3.9/site-packages/torch/lib/libtorch_cpu.so: undefined symbol: iJIT_NotifyEvent",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Cell 1: Imports and Setup\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModel\n",
      "File \u001b[0;32m/nas/ucb/kishorechidambaram/anaconda3/envs/mamba_env/lib/python3.9/site-packages/torch/__init__.py:202\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m USE_GLOBAL_DEPS:\n\u001b[1;32m    201\u001b[0m         _load_global_deps()\n\u001b[0;32m--> 202\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;66;03m# Appease the type checker; ordinarily this binding is inserted by the\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;66;03m# torch._C module initialization code in C\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
      "\u001b[0;31mImportError\u001b[0m: /nas/ucb/kishorechidambaram/anaconda3/envs/mamba_env/lib/python3.9/site-packages/torch/lib/libtorch_cpu.so: undefined symbol: iJIT_NotifyEvent"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports and Setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from mamba_ssm import Mamba\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Dataset Class\n",
    "class DNASequenceAndEmbeddingDataset(Dataset):\n",
    "    def __init__(self, sequences, labels, tokenizer, bert_model, device, max_length=128):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.bert_model = bert_model\n",
    "        self.max_length = max_length\n",
    "        self.device = device\n",
    "        \n",
    "        # Nucleotide to index mapping\n",
    "        self.nuc_to_idx = {'A': 0, 'C': 1, 'G': 2, 'T': 3, 'N': 4, 'PAD': 5}\n",
    "        \n",
    "        # Pre-compute embeddings and sequence encodings\n",
    "        self.embeddings = []\n",
    "        self.sequence_encodings = []\n",
    "        self.bert_model.eval()\n",
    "        \n",
    "        print(\"Computing embeddings and sequence encodings...\")\n",
    "        with torch.no_grad():\n",
    "            for seq in tqdm(sequences):\n",
    "                # Get BERT embeddings\n",
    "                inputs = tokenizer(\n",
    "                    seq,\n",
    "                    padding=\"max_length\",\n",
    "                    truncation=True,\n",
    "                    max_length=self.max_length,\n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "                inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "                outputs = self.bert_model(**inputs)\n",
    "                embeddings = outputs.last_hidden_state.cpu()\n",
    "                \n",
    "                # Create sequence encoding\n",
    "                seq_encoding = torch.tensor([\n",
    "                    self.nuc_to_idx.get(nuc, self.nuc_to_idx['N']) \n",
    "                    for nuc in (seq[:max_length] + 'P' * max(0, max_length - len(seq)))\n",
    "                ], dtype=torch.long)\n",
    "                \n",
    "                self.embeddings.append(embeddings.squeeze(0))\n",
    "                self.sequence_encodings.append(seq_encoding)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'embeddings': self.embeddings[idx],\n",
    "            'sequence': self.sequence_encodings[idx],\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Model Architecture\n",
    "class DNABertMambaWithSequenceClassifier(nn.Module):\n",
    "    def __init__(self, d_model=768, n_classes=3, d_state=16, d_conv=4, expand=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Nucleotide embedding\n",
    "        self.nuc_embedding = nn.Embedding(6, 32)  # 6 tokens (ACGTN + PAD), 32 dimensions\n",
    "        \n",
    "        # Combine sequence and BERT embeddings\n",
    "        self.combine_layer = nn.Sequential(\n",
    "            nn.Linear(d_model + 32, d_model),\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Mamba configuration\n",
    "        self.mamba = Mamba(\n",
    "            d_model=d_model,\n",
    "            d_state=d_state,\n",
    "            d_conv=d_conv,\n",
    "            expand=expand,\n",
    "            use_fast_path=True  # Added for PyTorch 1.12.0 compatibility\n",
    "        )\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(d_model, n_classes)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "    def forward(self, embeddings, sequence, labels=None):\n",
    "        # Get nucleotide embeddings\n",
    "        seq_embeddings = self.nuc_embedding(sequence)\n",
    "        \n",
    "        # Combine BERT embeddings with sequence embeddings\n",
    "        combined = torch.cat([embeddings, seq_embeddings], dim=-1)\n",
    "        x = self.combine_layer(combined)\n",
    "        \n",
    "        # Pass through Mamba\n",
    "        x = self.mamba(x)\n",
    "        \n",
    "        # Global average pooling\n",
    "        x = x.mean(dim=1)\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(x)\n",
    "        \n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits, labels)\n",
    "            return loss, logits\n",
    "        return None, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Data Preparation and Model Initialization\n",
    "def prepare_model_and_data(train_df, test_df, sample_size=5000):\n",
    "    # Sample the data\n",
    "    train_sample_size = min(sample_size, len(train_df))\n",
    "    test_sample_size = min(sample_size//5, len(test_df))\n",
    "    \n",
    "    # Stratified sampling\n",
    "    train_df_sample = train_df.groupby('kind', group_keys=False).apply(\n",
    "        lambda x: x.sample(n=min(len(x), train_sample_size // 3))\n",
    "    ).reset_index(drop=True)\n",
    "    \n",
    "    test_df_sample = test_df.groupby('kind', group_keys=False).apply(\n",
    "        lambda x: x.sample(n=min(len(x), test_sample_size // 3))\n",
    "    ).reset_index(drop=True)\n",
    "    \n",
    "    # Initialize BERT\n",
    "    model_name = \"zhihan1996/DNABERT-2-117M\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    bert_model = AutoModel.from_pretrained(model_name, trust_remote_code=True)\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Freeze BERT and move to device\n",
    "    for param in bert_model.parameters():\n",
    "        param.requires_grad = False\n",
    "    bert_model = bert_model.to(device)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = DNASequenceAndEmbeddingDataset(\n",
    "        sequences=train_df_sample['sequence'].tolist(),\n",
    "        labels=train_df_sample['kind'].tolist(),\n",
    "        tokenizer=tokenizer,\n",
    "        bert_model=bert_model,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    test_dataset = DNASequenceAndEmbeddingDataset(\n",
    "        sequences=test_df_sample['sequence'].tolist(),\n",
    "        labels=test_df_sample['kind'].tolist(),\n",
    "        tokenizer=tokenizer,\n",
    "        bert_model=bert_model,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        num_workers=2,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=False,\n",
    "        num_workers=2,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Initialize model\n",
    "    model = DNABertMambaWithSequenceClassifier(\n",
    "        d_model=768,\n",
    "        n_classes=3,\n",
    "        d_state=16,\n",
    "        d_conv=4,\n",
    "        expand=2\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    \n",
    "    return model, train_dataloader, test_dataloader, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Training Function\n",
    "def train_model(model, train_dataloader, val_dataloader, device, \n",
    "                num_epochs=10,\n",
    "                patience=3,\n",
    "                learning_rate=1e-4):\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=2, verbose=True\n",
    "    )\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    training_history = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        train_preds = []\n",
    "        train_labels = []\n",
    "        \n",
    "        progress_bar = tqdm(train_dataloader, desc=f'Epoch {epoch+1}/{num_epochs} - Training')\n",
    "        for batch in progress_bar:\n",
    "            embeddings = batch['embeddings'].to(device)\n",
    "            sequence = batch['sequence'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss, logits = model(embeddings, sequence, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Added gradient clipping\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            train_preds.extend(predictions.cpu().numpy())\n",
    "            train_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "            progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_dataloader, desc='Validation'):\n",
    "                embeddings = batch['embeddings'].to(device)\n",
    "                sequence = batch['sequence'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                \n",
    "                loss, logits = model(embeddings, sequence, labels)\n",
    "                \n",
    "                total_val_loss += loss.item()\n",
    "                predictions = torch.argmax(logits, dim=-1)\n",
    "                val_preds.extend(predictions.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # Calculate metrics\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "        avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "        train_acc = accuracy_score(train_labels, train_preds)\n",
    "        val_acc = accuracy_score(val_labels, val_preds)\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        print(f'\\nEpoch {epoch+1}:')\n",
    "        print(f'Training Loss: {avg_train_loss:.4f} | Training Accuracy: {train_acc:.4f}')\n",
    "        print(f'Validation Loss: {avg_val_loss:.4f} | Validation Accuracy: {val_acc:.4f}')\n",
    "        print('\\nValidation Classification Report:')\n",
    "        print(classification_report(val_labels, val_preds))\n",
    "        \n",
    "        training_history.append({\n",
    "            'epoch': epoch+1,\n",
    "            'train_loss': avg_train_loss,\n",
    "            'train_acc': train_acc,\n",
    "            'val_loss': avg_val_loss,\n",
    "            'val_acc': val_acc\n",
    "        })\n",
    "        \n",
    "        # Early stopping\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': best_val_loss,\n",
    "            }, 'best_mamba_model.pt')\n",
    "            print(\"Saved new best model!\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping triggered after epoch {epoch+1}\")\n",
    "                break\n",
    "    \n",
    "    return pd.DataFrame(training_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Run the Pipeline\n",
    "# Initialize model and data\n",
    "model, train_dataloader, test_dataloader, device = prepare_model_and_data(\n",
    "    train_df=train_df,\n",
    "    test_df=test_df,\n",
    "    sample_size=5000\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history_df = train_model(\n",
    "    model=model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloader=test_dataloader,\n",
    "    device=device,\n",
    "    num_epochs=10,\n",
    "    patience=3,\n",
    "    learning_rate=1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_df['train_loss'], label='Train Loss')\n",
    "plt.plot(history_df['val_loss'], label='Val Loss')\n",
    "plt.title('Loss Over Time')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_df['train_acc'], label='Train Acc')\n",
    "plt.plot(history_df['val_acc'], label='Val Acc')\n",
    "plt.title('Accuracy Over Time')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mamba_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
