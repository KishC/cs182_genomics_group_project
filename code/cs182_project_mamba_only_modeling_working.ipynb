{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip uninstall torch torchvision torchaudio transformers mamba-ssm tqdm pandas scikit-learn numpy -y\n",
    "# %pip cache purge\n",
    "# %pip install peft torch transformers mamba-ssm tqdm pandas scikit-learn numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>coord</th>\n",
       "      <th>kind</th>\n",
       "      <th>transcript</th>\n",
       "      <th>strand</th>\n",
       "      <th>chrom</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>sequence</th>\n",
       "      <th>win_start</th>\n",
       "      <th>win_end</th>\n",
       "      <th>is_truncated</th>\n",
       "      <th>motif_len</th>\n",
       "      <th>motif</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NC_050096.1_35318_donor</td>\n",
       "      <td>35318</td>\n",
       "      <td>0</td>\n",
       "      <td>XM_020544715.3</td>\n",
       "      <td>+</td>\n",
       "      <td>NC_050096.1</td>\n",
       "      <td>35319</td>\n",
       "      <td>35320</td>\n",
       "      <td>GGGCCCGGCTGGGCCTCAGCGGGGTCGTCGAGATGGAGATGGGGAG...</td>\n",
       "      <td>35118</td>\n",
       "      <td>35520</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>GT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NC_050096.1_34607_acceptor</td>\n",
       "      <td>34607</td>\n",
       "      <td>1</td>\n",
       "      <td>XM_020544715.3</td>\n",
       "      <td>+</td>\n",
       "      <td>NC_050096.1</td>\n",
       "      <td>34605</td>\n",
       "      <td>34606</td>\n",
       "      <td>TCCGGTGATTAATTTGTCCTTATACCTTTACAACAAAAATTCACTA...</td>\n",
       "      <td>34404</td>\n",
       "      <td>34806</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>TG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NC_050096.1_36174_donor</td>\n",
       "      <td>36174</td>\n",
       "      <td>0</td>\n",
       "      <td>XM_020544715.3</td>\n",
       "      <td>+</td>\n",
       "      <td>NC_050096.1</td>\n",
       "      <td>36175</td>\n",
       "      <td>36176</td>\n",
       "      <td>ATAATATGTTCATTATATCACAACACTCTTTTCTTATGGAGTCGTG...</td>\n",
       "      <td>35974</td>\n",
       "      <td>36376</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>GT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NC_050096.1_36037_acceptor</td>\n",
       "      <td>36037</td>\n",
       "      <td>1</td>\n",
       "      <td>XM_020544715.3</td>\n",
       "      <td>+</td>\n",
       "      <td>NC_050096.1</td>\n",
       "      <td>36035</td>\n",
       "      <td>36036</td>\n",
       "      <td>GCACAAAACTAACTAAAGGAATCATTCTGATAGATAACACTATAAA...</td>\n",
       "      <td>35834</td>\n",
       "      <td>36236</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>AG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NC_050096.1_36504_donor</td>\n",
       "      <td>36504</td>\n",
       "      <td>0</td>\n",
       "      <td>XM_020544715.3</td>\n",
       "      <td>+</td>\n",
       "      <td>NC_050096.1</td>\n",
       "      <td>36505</td>\n",
       "      <td>36506</td>\n",
       "      <td>TGTCATTTCCTTACCTCATTGAATCATTTCCGATGCTTCTTCTCTG...</td>\n",
       "      <td>36304</td>\n",
       "      <td>36706</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>GT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           id  coord  kind      transcript strand  \\\n",
       "0     NC_050096.1_35318_donor  35318     0  XM_020544715.3      +   \n",
       "1  NC_050096.1_34607_acceptor  34607     1  XM_020544715.3      +   \n",
       "2     NC_050096.1_36174_donor  36174     0  XM_020544715.3      +   \n",
       "3  NC_050096.1_36037_acceptor  36037     1  XM_020544715.3      +   \n",
       "4     NC_050096.1_36504_donor  36504     0  XM_020544715.3      +   \n",
       "\n",
       "         chrom  start    end  \\\n",
       "0  NC_050096.1  35319  35320   \n",
       "1  NC_050096.1  34605  34606   \n",
       "2  NC_050096.1  36175  36176   \n",
       "3  NC_050096.1  36035  36036   \n",
       "4  NC_050096.1  36505  36506   \n",
       "\n",
       "                                            sequence  win_start  win_end  \\\n",
       "0  GGGCCCGGCTGGGCCTCAGCGGGGTCGTCGAGATGGAGATGGGGAG...      35118    35520   \n",
       "1  TCCGGTGATTAATTTGTCCTTATACCTTTACAACAAAAATTCACTA...      34404    34806   \n",
       "2  ATAATATGTTCATTATATCACAACACTCTTTTCTTATGGAGTCGTG...      35974    36376   \n",
       "3  GCACAAAACTAACTAAAGGAATCATTCTGATAGATAACACTATAAA...      35834    36236   \n",
       "4  TGTCATTTCCTTACCTCATTGAATCATTTCCGATGCTTCTTCTCTG...      36304    36706   \n",
       "\n",
       "   is_truncated  motif_len motif  \n",
       "0         False          2    GT  \n",
       "1         False          2    TG  \n",
       "2         False          2    GT  \n",
       "3         False          2    AG  \n",
       "4         False          2    GT  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splice_df = pd.read_csv('splice_sites_full_centered_balanced_correct_V3.csv')\n",
    "splice_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data with stratification\n",
    "train_df, temp_df = train_test_split(splice_df, test_size=0.3, stratify=splice_df['kind'], random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['kind'], random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0+cu124\n",
      "CUDA available: True\n",
      "CUDA version: 12.4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from mamba_ssm import Mamba\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Dataset Class (Modified)\n",
    "class DNASequenceDataset(Dataset):\n",
    "    def __init__(self, sequences, labels, max_length=512):\n",
    "        self.sequences = sequences\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "        self.max_length = max_length\n",
    "        self.nuc_to_idx = {'A': 0, 'C': 1, 'G': 2, 'T': 3, 'N': 4, 'P': 5}\n",
    "        \n",
    "        self.seq_encodings = []\n",
    "        for seq in sequences:\n",
    "            encoding = [self.nuc_to_idx.get(nuc, self.nuc_to_idx['N']) for nuc in seq[:max_length]]\n",
    "            encoding += [self.nuc_to_idx['P']] * (max_length - len(encoding))\n",
    "            self.seq_encodings.append(torch.tensor(encoding, dtype=torch.long))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'sequence': self.seq_encodings[idx],\n",
    "            'label': self.labels[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MambaSequenceClassifier(nn.Module):\n",
    "    def __init__(self, d_model=128, d_state=16, d_conv=4, expand=2, num_mamba_layers=4, n_classes=3, max_length=512):\n",
    "        super().__init__()\n",
    "        self.nuc_embedding = nn.Embedding(6, d_model)  # ACGTNP to d_model\n",
    "        self.pos_embedding = nn.Embedding(max_length, d_model)  # Learnable positional embeddings\n",
    "\n",
    "        self.mamba = nn.Sequential(*[\n",
    "            Mamba(\n",
    "                d_model=d_model,\n",
    "                d_state=d_state,\n",
    "                d_conv=d_conv,\n",
    "                expand=expand,\n",
    "                use_fast_path=True\n",
    "            ) for _ in range(num_mamba_layers)\n",
    "        ])\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(d_model, n_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, sequence, labels=None):\n",
    "        pos_ids = torch.arange(sequence.size(1), device=sequence.device).unsqueeze(0).expand(sequence.size(0), -1)\n",
    "        x = self.nuc_embedding(sequence) + self.pos_embedding(pos_ids)  # Add position to nucleotide embeddings\n",
    "        x = self.mamba(x)\n",
    "        x = x.mean(dim=1)  # Global average pooling\n",
    "        logits = self.classifier(x)\n",
    "        \n",
    "        if labels is not None:\n",
    "            loss = nn.CrossEntropyLoss()(logits, labels)\n",
    "            return loss, logits\n",
    "        return None, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Model Initialization (Modified)\n",
    "def prepare_model_and_data(train_df, test_df, sample_size=5000, max_length=512):\n",
    "    train_sample = train_df.groupby('kind', group_keys=False).apply(\n",
    "        lambda x: x.sample(n=min(len(x), sample_size // 3))\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    test_sample = test_df.groupby('kind', group_keys=False).apply(\n",
    "        lambda x: x.sample(n=min(len(x), sample_size // 15))\n",
    "    ).reset_index(drop=True)\n",
    "    \n",
    "    train_dataset = DNASequenceDataset(\n",
    "        sequences=train_sample['sequence'].tolist(),\n",
    "        labels=train_sample['kind'].tolist(),\n",
    "        max_length=max_length\n",
    "    )\n",
    "    \n",
    "    test_dataset = DNASequenceDataset(\n",
    "        sequences=test_sample['sequence'].tolist(),\n",
    "        labels=test_sample['kind'].tolist(),\n",
    "        max_length=max_length\n",
    "    )\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=32)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = MambaSequenceClassifier(\n",
    "        d_model=128,\n",
    "        d_state=16,\n",
    "        d_conv=4,\n",
    "        expand=1,\n",
    "        num_mamba_layers=4,\n",
    "        n_classes=3\n",
    "    ).to(device)\n",
    "    \n",
    "    return model, train_dataloader, test_dataloader, device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Training Function (Modified)\n",
    "def train_model(model, train_dataloader, val_dataloader, device, \n",
    "                num_epochs=10, patience=3, learning_rate=1e-4):\n",
    "    \n",
    "    # Initialize optimizer and tracking variables\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    # Initialize tracking lists\n",
    "    history = {\n",
    "        'epoch': [], 'train_loss': [], 'val_loss': [],\n",
    "        'train_acc': [], 'val_acc': [], 'learning_rate': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for batch in tqdm(train_dataloader, desc=f'Epoch {epoch + 1}/{num_epochs}'):\n",
    "            sequence = batch['sequence'].to(device)\n",
    "            labels = batch['label'].to(device)  # Changed from 'labels' to 'label'\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss, logits = model(sequence, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            train_correct += (predictions == labels).sum().item()\n",
    "            train_total += labels.size(0)\n",
    "        \n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "        train_accuracy = train_correct / train_total\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_dataloader:\n",
    "                sequence = batch['sequence'].to(device)\n",
    "                labels = batch['label'].to(device)  # Changed from 'labels' to 'label'\n",
    "                \n",
    "                loss, logits = model(sequence, labels)\n",
    "                total_val_loss += loss.item()\n",
    "                \n",
    "                # Calculate accuracy\n",
    "                predictions = torch.argmax(logits, dim=1)\n",
    "                val_correct += (predictions == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "        \n",
    "        avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "        val_accuracy = val_correct / val_total\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step(avg_val_loss)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # Save history\n",
    "        history['epoch'].append(epoch + 1)\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['train_acc'].append(train_accuracy)\n",
    "        history['val_acc'].append(val_accuracy)\n",
    "        history['learning_rate'].append(current_lr)\n",
    "        \n",
    "        # Print metrics\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}:')\n",
    "        print(f'Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.4f}')\n",
    "        print(f'Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.4f}')\n",
    "        print(f'Learning Rate: {current_lr}')\n",
    "        \n",
    "        # Early stopping check\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        if patience_counter >= patience:\n",
    "            print(f'Early stopping triggered after epoch {epoch + 1}')\n",
    "            break\n",
    "    \n",
    "    # Load best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    # Convert history to DataFrame\n",
    "    history_df = pd.DataFrame(history)\n",
    "    return history_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "385ed8646aa5430eac130bc87663621b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/20:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20:\n",
      "Train Loss: 1.0958, Train Acc: 0.3499\n",
      "Val Loss: 1.0362, Val Acc: 0.4454\n",
      "Learning Rate: 0.0001\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "865700d322224e5eb46e7fd143f32abe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/20:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20:\n",
      "Train Loss: 1.0316, Train Acc: 0.4538\n",
      "Val Loss: 1.0096, Val Acc: 0.4685\n",
      "Learning Rate: 0.0001\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52312af8393e4865b34d566cf5137488",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/20:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20:\n",
      "Train Loss: 1.0017, Train Acc: 0.4644\n",
      "Val Loss: 0.9842, Val Acc: 0.4795\n",
      "Learning Rate: 0.0001\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32f7847b33674f33b15f5d0d4707079e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/20:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20:\n",
      "Train Loss: 0.9621, Train Acc: 0.4954\n",
      "Val Loss: 0.9564, Val Acc: 0.5125\n",
      "Learning Rate: 0.0001\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc762fab36984264a8b3a41f881e3648",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/20:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20:\n",
      "Train Loss: 0.8356, Train Acc: 0.6182\n",
      "Val Loss: 0.7748, Val Acc: 0.7077\n",
      "Learning Rate: 0.0001\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb94cccd63be48c79bdecd530fda5789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/20:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20:\n",
      "Train Loss: 0.6978, Train Acc: 0.7181\n",
      "Val Loss: 0.7459, Val Acc: 0.6937\n",
      "Learning Rate: 0.0001\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d4db2fb0e4f4758aa339824622cfc6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/20:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20:\n",
      "Train Loss: 0.6668, Train Acc: 0.7365\n",
      "Val Loss: 0.7049, Val Acc: 0.7157\n",
      "Learning Rate: 0.0001\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1c856c39cce46039acbfc9012266ce8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/20:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20:\n",
      "Train Loss: 0.6055, Train Acc: 0.7649\n",
      "Val Loss: 0.6027, Val Acc: 0.7808\n",
      "Learning Rate: 0.0001\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52808b1a33c14bb1991fa425c0285fc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/20:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20:\n",
      "Train Loss: 0.4994, Train Acc: 0.8039\n",
      "Val Loss: 0.5523, Val Acc: 0.7988\n",
      "Learning Rate: 0.0001\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "916e58b18e63461680c2e3640af5e6e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/20:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20:\n",
      "Train Loss: 0.4500, Train Acc: 0.8235\n",
      "Val Loss: 0.5062, Val Acc: 0.8098\n",
      "Learning Rate: 0.0001\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e4a55d04a394016a65a01cefef66d24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11/20:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20:\n",
      "Train Loss: 0.4259, Train Acc: 0.8335\n",
      "Val Loss: 0.5443, Val Acc: 0.7788\n",
      "Learning Rate: 0.0001\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe554c5c7126404899abfc4750a2cd8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 12/20:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20:\n",
      "Train Loss: 0.4095, Train Acc: 0.8349\n",
      "Val Loss: 0.4891, Val Acc: 0.7958\n",
      "Learning Rate: 0.0001\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9768e479cfca4a3fa4c2781c3c49fc7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 13/20:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20:\n",
      "Train Loss: 0.3964, Train Acc: 0.8417\n",
      "Val Loss: 0.4951, Val Acc: 0.8158\n",
      "Learning Rate: 0.0001\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13afd6c582d84a5896da43dd5ae55df0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 14/20:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20:\n",
      "Train Loss: 0.3751, Train Acc: 0.8477\n",
      "Val Loss: 0.4481, Val Acc: 0.8238\n",
      "Learning Rate: 0.0001\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ec7c750dd3048b6848de6c93870595e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 15/20:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20:\n",
      "Train Loss: 0.3544, Train Acc: 0.8609\n",
      "Val Loss: 0.4670, Val Acc: 0.8378\n",
      "Learning Rate: 0.0001\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5102fc54a6374037971fcdc264855152",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 16/20:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20:\n",
      "Train Loss: 0.3392, Train Acc: 0.8703\n",
      "Val Loss: 0.4571, Val Acc: 0.8438\n",
      "Learning Rate: 0.0001\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4515ae0a32e4caeb97ffaf1d588b3e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 17/20:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20:\n",
      "Train Loss: 0.3141, Train Acc: 0.8878\n",
      "Val Loss: 0.4257, Val Acc: 0.8689\n",
      "Learning Rate: 0.0001\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fda8275c52c04b25989e629f1937f99b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 18/20:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20:\n",
      "Train Loss: 0.3056, Train Acc: 0.8886\n",
      "Val Loss: 0.4260, Val Acc: 0.8649\n",
      "Learning Rate: 0.0001\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "970f11fd6e8e47d9aa027a05408cbccf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 19/20:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20:\n",
      "Train Loss: 0.3005, Train Acc: 0.8916\n",
      "Val Loss: 0.3977, Val Acc: 0.8599\n",
      "Learning Rate: 0.0001\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f01cf3420fe34833817673efba55ee5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 20/20:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20:\n",
      "Train Loss: 0.2913, Train Acc: 0.8970\n",
      "Val Loss: 0.4117, Val Acc: 0.8579\n",
      "Learning Rate: 0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Cell 6: Run the Pipeline\n",
    "# Initialize model and data\n",
    "model, train_dataloader, test_dataloader, device = prepare_model_and_data(\n",
    "    train_df=train_df,\n",
    "    test_df=test_df,\n",
    "    sample_size=5000\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history_df = train_model(\n",
    "    model=model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloader=test_dataloader,\n",
    "    device=device,\n",
    "    num_epochs=20,\n",
    "    patience=10,\n",
    "    learning_rate=1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_df['train_loss'], label='Train Loss')\n",
    "plt.plot(history_df['val_loss'], label='Val Loss')\n",
    "plt.title('Loss Over Time')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_df['train_acc'], label='Train Acc')\n",
    "plt.plot(history_df['val_acc'], label='Val Acc')\n",
    "plt.title('Accuracy Over Time')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Compute Test Accuracy\n",
    "def compute_test_accuracy(model, test_dataloader, device):\n",
    "    model.eval()\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            sequence = batch['sequence'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            _, logits = model(sequence)\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            test_correct += (predictions == labels).sum().item()\n",
    "            test_total += labels.size(0)\n",
    "    \n",
    "    test_accuracy = test_correct / test_total\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Compute test accuracy\n",
    "compute_test_accuracy(model, test_dataloader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NERSC Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
