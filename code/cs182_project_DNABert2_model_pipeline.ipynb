{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 3114,
     "status": "ok",
     "timestamp": 1744858432775,
     "user": {
      "displayName": "Kishore Chidambaram",
      "userId": "09064313255579917314"
     },
     "user_tz": 420
    },
    "id": "AA9niyW3HiFL",
    "outputId": "bc2d2e86-1f43-45ad-d01e-eb64b38fa4d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: triton 2.2.0\n",
      "Uninstalling triton-2.2.0:\n",
      "  Successfully uninstalled triton-2.2.0\n",
      "Found existing installation: torch 2.2.0\n",
      "Uninstalling torch-2.2.0:\n",
      "  Successfully uninstalled torch-2.2.0\n",
      "\u001b[33mWARNING: Skipping torchvision as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping torchaudio as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mFound existing installation: transformers 4.51.3\n",
      "Uninstalling transformers-4.51.3:\n",
      "  Successfully uninstalled transformers-4.51.3\n",
      "Files removed: 84\n",
      "Collecting torch==2.2.0\n",
      "  Downloading torch-2.2.0-cp312-cp312-manylinux1_x86_64.whl.metadata (25 kB)\n",
      "Requirement already satisfied: filelock in /nas/ucb/kishorechidambaram/anaconda3/lib/python3.12/site-packages (from torch==2.2.0) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /nas/ucb/kishorechidambaram/anaconda3/lib/python3.12/site-packages (from torch==2.2.0) (4.11.0)\n",
      "Requirement already satisfied: sympy in /nas/ucb/kishorechidambaram/anaconda3/lib/python3.12/site-packages (from torch==2.2.0) (1.13.1)\n",
      "Requirement already satisfied: networkx in /nas/ucb/kishorechidambaram/anaconda3/lib/python3.12/site-packages (from torch==2.2.0) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /nas/ucb/kishorechidambaram/anaconda3/lib/python3.12/site-packages (from torch==2.2.0) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /nas/ucb/kishorechidambaram/anaconda3/lib/python3.12/site-packages (from torch==2.2.0) (2024.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /nas/ucb/kishorechidambaram/anaconda3/lib/python3.12/site-packages (from torch==2.2.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /nas/ucb/kishorechidambaram/anaconda3/lib/python3.12/site-packages (from torch==2.2.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /nas/ucb/kishorechidambaram/anaconda3/lib/python3.12/site-packages (from torch==2.2.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /nas/ucb/kishorechidambaram/anaconda3/lib/python3.12/site-packages (from torch==2.2.0) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /nas/ucb/kishorechidambaram/anaconda3/lib/python3.12/site-packages (from torch==2.2.0) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /nas/ucb/kishorechidambaram/anaconda3/lib/python3.12/site-packages (from torch==2.2.0) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /nas/ucb/kishorechidambaram/anaconda3/lib/python3.12/site-packages (from torch==2.2.0) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /nas/ucb/kishorechidambaram/anaconda3/lib/python3.12/site-packages (from torch==2.2.0) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /nas/ucb/kishorechidambaram/anaconda3/lib/python3.12/site-packages (from torch==2.2.0) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /nas/ucb/kishorechidambaram/anaconda3/lib/python3.12/site-packages (from torch==2.2.0) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /nas/ucb/kishorechidambaram/anaconda3/lib/python3.12/site-packages (from torch==2.2.0) (12.1.105)\n",
      "Collecting triton==2.2.0 (from torch==2.2.0)\n",
      "  Downloading triton-2.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /nas/ucb/kishorechidambaram/anaconda3/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.0) (12.4.127)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /nas/ucb/kishorechidambaram/anaconda3/lib/python3.12/site-packages (from jinja2->torch==2.2.0) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /nas/ucb/kishorechidambaram/anaconda3/lib/python3.12/site-packages (from sympy->torch==2.2.0) (1.3.0)\n",
      "Downloading torch-2.2.0-cp312-cp312-manylinux1_x86_64.whl (755.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.4/755.4 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading triton-2.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: triton, torch\n",
      "Successfully installed torch-2.2.0 triton-2.2.0\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: filelock in /nas/ucb/kishorechidambaram/anaconda3/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /nas/ucb/kishorechidambaram/anaconda3/lib/python3.12/site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /nas/ucb/kishorechidambaram/anaconda3/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /nas/ucb/kishorechidambaram/anaconda3/lib/python3.12/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /nas/ucb/kishorechidambaram/anaconda3/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /nas/ucb/kishorechidambaram/anaconda3/lib/python3.12/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /nas/ucb/kishorechidambaram/anaconda3/lib/python3.12/site-packages (from transformers) (2.32.2)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /nas/ucb/kishorechidambaram/anaconda3/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /nas/ucb/kishorechidambaram/anaconda3/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /nas/ucb/kishorechidambaram/anaconda3/lib/python3.12/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /nas/ucb/kishorechidambaram/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /nas/ucb/kishorechidambaram/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /nas/ucb/kishorechidambaram/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /nas/ucb/kishorechidambaram/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /nas/ucb/kishorechidambaram/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /nas/ucb/kishorechidambaram/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2025.1.31)\n",
      "Downloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: transformers\n",
      "Successfully installed transformers-4.51.3\n"
     ]
    }
   ],
   "source": [
    "#run in terminal\n",
    "# !pip uninstall -y triton\n",
    "# !pip uninstall -y torch torchvision torchaudio\n",
    "# !pip uninstall -y transformers\n",
    "# !pip cache purge\n",
    "# !pip install torch==2.2.0\n",
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 13336,
     "status": "ok",
     "timestamp": 1744858446162,
     "user": {
      "displayName": "Kishore Chidambaram",
      "userId": "09064313255579917314"
     },
     "user_tz": 420
    },
    "id": "oDg3_Od-Hk3x"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
    "from datasets import Dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Splice Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 159
    },
    "executionInfo": {
     "elapsed": 49,
     "status": "error",
     "timestamp": 1744858858140,
     "user": {
      "displayName": "Kishore Chidambaram",
      "userId": "09064313255579917314"
     },
     "user_tz": 420
    },
    "id": "Bs_z-X01qB-r",
    "outputId": "49444ff3-2ba4-4d6b-a31d-874af8d7640e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>coord</th>\n",
       "      <th>kind</th>\n",
       "      <th>transcript</th>\n",
       "      <th>strand</th>\n",
       "      <th>chrom</th>\n",
       "      <th>sequence</th>\n",
       "      <th>motif</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NC_050096.1_35318_donor</td>\n",
       "      <td>35318</td>\n",
       "      <td>0</td>\n",
       "      <td>ID=exon-XM_020544715.3-1</td>\n",
       "      <td>+</td>\n",
       "      <td>NC_050096.1</td>\n",
       "      <td>GGGCCCGGCTGGGCCTCAGCGGGGTCGTCGAGATGGAGATGGGGAG...</td>\n",
       "      <td>GT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NC_050096.1_36174_donor</td>\n",
       "      <td>36174</td>\n",
       "      <td>0</td>\n",
       "      <td>ID=exon-XM_020544715.3-2</td>\n",
       "      <td>+</td>\n",
       "      <td>NC_050096.1</td>\n",
       "      <td>ATAATATGTTCATTATATCACAACACTCTTTTCTTATGGAGTCGTG...</td>\n",
       "      <td>GT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NC_050096.1_36504_donor</td>\n",
       "      <td>36504</td>\n",
       "      <td>0</td>\n",
       "      <td>ID=exon-XM_020544715.3-3</td>\n",
       "      <td>+</td>\n",
       "      <td>NC_050096.1</td>\n",
       "      <td>TGTCATTTCCTTACCTCATTGAATCATTTCCGATGCTTCTTCTCTG...</td>\n",
       "      <td>GT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NC_050096.1_36713_donor</td>\n",
       "      <td>36713</td>\n",
       "      <td>0</td>\n",
       "      <td>ID=exon-XM_020544715.3-4</td>\n",
       "      <td>+</td>\n",
       "      <td>NC_050096.1</td>\n",
       "      <td>TTCATGGTAGTCATTGGAACCTGCTAGATTGTACACTTGACAATAA...</td>\n",
       "      <td>GT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NC_050096.1_37004_donor</td>\n",
       "      <td>37004</td>\n",
       "      <td>0</td>\n",
       "      <td>ID=exon-XM_020544715.3-5</td>\n",
       "      <td>+</td>\n",
       "      <td>NC_050096.1</td>\n",
       "      <td>CAACTTTTTCCTTTCAGATTTCCAGTACAGTCCTCGCTATTGCTGT...</td>\n",
       "      <td>GT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        id  coord  kind                transcript strand  \\\n",
       "0  NC_050096.1_35318_donor  35318     0  ID=exon-XM_020544715.3-1      +   \n",
       "1  NC_050096.1_36174_donor  36174     0  ID=exon-XM_020544715.3-2      +   \n",
       "2  NC_050096.1_36504_donor  36504     0  ID=exon-XM_020544715.3-3      +   \n",
       "3  NC_050096.1_36713_donor  36713     0  ID=exon-XM_020544715.3-4      +   \n",
       "4  NC_050096.1_37004_donor  37004     0  ID=exon-XM_020544715.3-5      +   \n",
       "\n",
       "         chrom                                           sequence motif  \n",
       "0  NC_050096.1  GGGCCCGGCTGGGCCTCAGCGGGGTCGTCGAGATGGAGATGGGGAG...    GT  \n",
       "1  NC_050096.1  ATAATATGTTCATTATATCACAACACTCTTTTCTTATGGAGTCGTG...    GT  \n",
       "2  NC_050096.1  TGTCATTTCCTTACCTCATTGAATCATTTCCGATGCTTCTTCTCTG...    GT  \n",
       "3  NC_050096.1  TTCATGGTAGTCATTGGAACCTGCTAGATTGTACACTTGACAATAA...    GT  \n",
       "4  NC_050096.1  CAACTTTTTCCTTTCAGATTTCCAGTACAGTCCTCGCTATTGCTGT...    GT  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splice_df = pd.read_csv('splice_dataset_final.csv')\n",
    "splice_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 210
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1744858479336,
     "user": {
      "displayName": "Kishore Chidambaram",
      "userId": "09064313255579917314"
     },
     "user_tz": 420
    },
    "id": "ApQpIQFQreaw",
    "outputId": "74d71553-1e33-4228-a3fc-a4d95b07ab59"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "kind\n",
       "2    1018230\n",
       "0     509115\n",
       "1     509115\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splice_df[\"kind\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "executionInfo": {
     "elapsed": 55,
     "status": "error",
     "timestamp": 1744858853658,
     "user": {
      "displayName": "Kishore Chidambaram",
      "userId": "09064313255579917314"
     },
     "user_tz": 420
    },
    "id": "BrBpdyy50pEq",
    "outputId": "9b4ed0f3-2af6-41e1-f06a-9527070dd716"
   },
   "outputs": [],
   "source": [
    "#@title K-mer Tokenization\n",
    "\n",
    "def seq2kmer(seq, k=6):\n",
    "    \"\"\"Convert DNA sequence to k-mer tokens\"\"\"\n",
    "    return \" \".join([seq[i:i+k] for i in range(len(seq)-k+1)])\n",
    "\n",
    "# Assuming your DataFrame is loaded as df\n",
    "splice_df['kmer_6'] = splice_df['sequence'].apply(lambda x: seq2kmer(x, k=3))\n",
    "\n",
    "# Split data with stratification\n",
    "train_df, temp_df = train_test_split(splice_df, test_size=0.3, stratify=splice_df['kind'], random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['kind'], random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Imports and Setup\n",
    "# Cell 1: Imports and Setup\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    BertConfig\n",
    ")\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Model Class\n",
    "# Cell 2: Model Class (Simplified)\n",
    "class DNABERTClassifier(torch.nn.Module):\n",
    "    def __init__(self, model_name, num_labels, class_weights):\n",
    "        super().__init__()\n",
    "        self.config = BertConfig.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=num_labels\n",
    "        )\n",
    "        self.bert = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            config=self.config\n",
    "        )\n",
    "        self.loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        if labels is not None:\n",
    "            loss = self.loss_fn(outputs.logits, labels)\n",
    "            return loss, outputs.logits\n",
    "        return None, outputs.logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Dataset Class, Data Preparation, and Training Function parts: Use the FULL DATASET version for full data. For most testing, use the SMALL DATASET version for faster runs (uses sample of data and another optimizations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Dataset Class [FULL DATASET]\n",
    "# [OLD VERSION] - FULL DATASET\n",
    "# Cell 3: Dataset Class\n",
    "class SpliceSiteDataset(Dataset):\n",
    "    def __init__(self, sequences, labels, tokenizer):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.sequences[idx],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        item = {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(self.labels[idx])\n",
    "        }\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Dataset Class [SMALL DATASET]\n",
    "# Cell 3: Dataset Class (Optimized)\n",
    "class SpliceSiteDataset(Dataset):\n",
    "    def __init__(self, sequences, labels, tokenizer, max_length=256):  # Reduced from 512\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.sequences[idx],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        item = {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(self.labels[idx])\n",
    "        }\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at zhihan1996/DNABERT-2-117M and are newly initialized: ['bert.embeddings.position_embeddings.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "#@title Model Initialization\n",
    "# Cell 4: Model Initialization (Modified)\n",
    "# Model name\n",
    "model_name = \"zhihan1996/DNABERT-2-117M\"\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Calculate class weights\n",
    "class_counts = train_df['kind'].value_counts().sort_index()\n",
    "class_weights = torch.tensor(\n",
    "    [class_counts.sum() / c for c in class_counts],\n",
    "    dtype=torch.float32\n",
    ")\n",
    "\n",
    "# Initialize model with specific configuration to avoid flash attention\n",
    "config = BertConfig.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=3,\n",
    "    attention_probs_dropout_prob=0.1,\n",
    "    hidden_dropout_prob=0.1,\n",
    "    use_cache=False\n",
    ")\n",
    "\n",
    "# Initialize model\n",
    "model = DNABERTClassifier(\n",
    "    model_name=model_name,\n",
    "    num_labels=3,\n",
    "    class_weights=class_weights\n",
    ")\n",
    "\n",
    "# Move model to device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Data Preparation [FULL DATASET]\n",
    "# Cell 5: Data Preparation\n",
    "# Create datasets\n",
    "train_dataset = SpliceSiteDataset(\n",
    "    sequences=train_df['sequence'].tolist(),\n",
    "    labels=train_df['kind'].tolist(),\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "test_dataset = SpliceSiteDataset(\n",
    "    sequences=test_df['sequence'].tolist(),\n",
    "    labels=test_df['kind'].tolist(),\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Data Preparation [SMALL DATASET]\n",
    "\n",
    "# Key optimizations made while maintaining stability:\n",
    "# Reduced sequence length from 512 to 128 tokens\n",
    "# Added data sampling (5000 training samples, 1000 test samples)\n",
    "# Increased batch size from 16 to 32\n",
    "# Added parallel data loading with num_workers=2\n",
    "# Added early stopping with patience=2\n",
    "# Added progress bars with loss updates\n",
    "# Added validation accuracy tracking\n",
    "# Improved progress monitoring and metrics display\n",
    "\n",
    "# Cell 5: Data Preparation (Optimized with sampling)\n",
    "# Sample the data for faster training\n",
    "train_sample_size = min(50000, len(train_df))  # Adjust this number as needed\n",
    "test_sample_size = min(10000, len(test_df))    # Adjust this number as needed\n",
    "\n",
    "train_df_sample = train_df.sample(n=train_sample_size, random_state=42)\n",
    "test_df_sample = test_df.sample(n=test_sample_size, random_state=42)\n",
    "\n",
    "# Create datasets with reduced sequence length\n",
    "train_dataset = SpliceSiteDataset(\n",
    "    sequences=train_df_sample['sequence'].tolist(),\n",
    "    labels=train_df_sample['kind'].tolist(),\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "test_dataset = SpliceSiteDataset(\n",
    "    sequences=test_df_sample['sequence'].tolist(),\n",
    "    labels=test_df_sample['kind'].tolist(),\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Create dataloaders with optimal batch size\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=32,  # Increased from 16\n",
    "    shuffle=True,\n",
    "    num_workers=2   # Added parallel data loading\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=32,  # Increased from 16\n",
    "    shuffle=False,\n",
    "    num_workers=2   # Added parallel data loading\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Training Function [FULL DATASET]\n",
    "# Cell 6: Training Function\n",
    "def train_model(model, train_dataloader, val_dataloader, device, num_epochs=3):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        \n",
    "        for batch in tqdm(train_dataloader, desc=f'Epoch {epoch+1}/{num_epochs}'):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss, _ = model(**batch)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        total_val_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_dataloader:\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                loss, logits = model(**batch)\n",
    "                \n",
    "                total_val_loss += loss.item()\n",
    "                predictions = torch.argmax(logits, dim=-1)\n",
    "                val_preds.extend(predictions.cpu().numpy())\n",
    "                val_labels.extend(batch['labels'].cpu().numpy())\n",
    "        \n",
    "        # Print metrics\n",
    "        print(f'\\nEpoch {epoch+1}:')\n",
    "        print(f'Average training loss: {total_train_loss/len(train_dataloader):.4f}')\n",
    "        print(f'Average validation loss: {total_val_loss/len(val_dataloader):.4f}')\n",
    "        print('\\nValidation Classification Report:')\n",
    "        print(classification_report(val_labels, val_preds))\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "        # Save best model\n",
    "        if total_val_loss < best_val_loss:\n",
    "            best_val_loss = total_val_loss\n",
    "            torch.save(model.state_dict(), 'best_splice_site_model.pt')\n",
    "            print(\"Saved new best model!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Training Function [SMALL DATASET]\n",
    "# MODIFIED VERSION - FASTER, SMALLER TRAIN\n",
    "# Cell 6: Training Function (Optimized)\n",
    "def train_model(model, train_dataloader, val_dataloader, device, \n",
    "                num_epochs=5,           # Increased from 3\n",
    "                patience=2,             # Added early stopping\n",
    "                learning_rate=2e-5):\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        \n",
    "        progress_bar = tqdm(train_dataloader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "        for batch in progress_bar:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss, _ = model(**batch)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "            progress_bar.set_postfix({'train_loss': f'{loss.item():.4f}'})\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        total_val_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_dataloader, desc='Validation'):\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                loss, logits = model(**batch)\n",
    "                \n",
    "                total_val_loss += loss.item()\n",
    "                predictions = torch.argmax(logits, dim=-1)\n",
    "                val_preds.extend(predictions.cpu().numpy())\n",
    "                val_labels.extend(batch['labels'].cpu().numpy())\n",
    "        \n",
    "        # Calculate average losses and accuracy\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "        avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "        val_accuracy = accuracy_score(val_labels, val_preds)\n",
    "        \n",
    "        # Print metrics\n",
    "        print(f'\\nEpoch {epoch+1}:')\n",
    "        print(f'Average training loss: {avg_train_loss:.4f}')\n",
    "        print(f'Average validation loss: {avg_val_loss:.4f}')\n",
    "        print(f'Validation accuracy: {val_accuracy:.4f}')\n",
    "        print('\\nValidation Classification Report:')\n",
    "        print(classification_report(val_labels, val_preds))\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "        # Save best model and early stopping\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), 'best_splice_site_model.pt')\n",
    "            print(\"Saved new best model!\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping triggered after epoch {epoch+1}\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:   0%|          | 0/1563 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 1/5: 100%|██████████| 1563/1563 [07:22<00:00,  3.53it/s, train_loss=0.6903]\n",
      "Validation:   0%|          | 0/313 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Validation: 100%|██████████| 313/313 [00:31<00:00,  9.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1:\n",
      "Average training loss: 0.8683\n",
      "Average validation loss: 0.8360\n",
      "Validation accuracy: 0.5927\n",
      "\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.32      0.09      0.15      2475\n",
      "           1       0.40      0.82      0.53      2464\n",
      "           2       0.88      0.73      0.80      5061\n",
      "\n",
      "    accuracy                           0.59     10000\n",
      "   macro avg       0.53      0.55      0.49     10000\n",
      "weighted avg       0.62      0.59      0.57     10000\n",
      "\n",
      "------------------------------------------------------------\n",
      "Saved new best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:   0%|          | 0/1563 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 2/5: 100%|██████████| 1563/1563 [07:23<00:00,  3.53it/s, train_loss=0.8140]\n",
      "Validation:   0%|          | 0/313 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Validation: 100%|██████████| 313/313 [00:31<00:00,  9.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2:\n",
      "Average training loss: 0.8260\n",
      "Average validation loss: 0.8219\n",
      "Validation accuracy: 0.6337\n",
      "\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.81      0.56      2475\n",
      "           1       0.47      0.05      0.09      2464\n",
      "           2       0.83      0.83      0.83      5061\n",
      "\n",
      "    accuracy                           0.63     10000\n",
      "   macro avg       0.58      0.56      0.49     10000\n",
      "weighted avg       0.64      0.63      0.58     10000\n",
      "\n",
      "------------------------------------------------------------\n",
      "Saved new best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:   0%|          | 0/1563 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 3/5: 100%|██████████| 1563/1563 [07:24<00:00,  3.52it/s, train_loss=0.9297]\n",
      "Validation:   0%|          | 0/313 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Validation: 100%|██████████| 313/313 [00:31<00:00,  9.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3:\n",
      "Average training loss: 0.7838\n",
      "Average validation loss: 0.7504\n",
      "Validation accuracy: 0.6722\n",
      "\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.74      0.57      2475\n",
      "           1       0.59      0.50      0.54      2464\n",
      "           2       0.92      0.72      0.81      5061\n",
      "\n",
      "    accuracy                           0.67     10000\n",
      "   macro avg       0.66      0.65      0.64     10000\n",
      "weighted avg       0.73      0.67      0.68     10000\n",
      "\n",
      "------------------------------------------------------------\n",
      "Saved new best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:   0%|          | 0/1563 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 4/5: 100%|██████████| 1563/1563 [07:23<00:00,  3.53it/s, train_loss=0.5156]\n",
      "Validation:   0%|          | 0/313 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Validation: 100%|██████████| 313/313 [00:31<00:00,  9.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4:\n",
      "Average training loss: 0.5800\n",
      "Average validation loss: 0.4806\n",
      "Validation accuracy: 0.8101\n",
      "\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.84      0.76      2475\n",
      "           1       0.77      0.82      0.79      2464\n",
      "           2       0.91      0.79      0.85      5061\n",
      "\n",
      "    accuracy                           0.81     10000\n",
      "   macro avg       0.79      0.82      0.80     10000\n",
      "weighted avg       0.82      0.81      0.81     10000\n",
      "\n",
      "------------------------------------------------------------\n",
      "Saved new best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:   0%|          | 0/1563 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 5/5: 100%|██████████| 1563/1563 [07:23<00:00,  3.53it/s, train_loss=0.2358]\n",
      "Validation:   0%|          | 0/313 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Validation: 100%|██████████| 313/313 [00:31<00:00,  9.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5:\n",
      "Average training loss: 0.4251\n",
      "Average validation loss: 0.5158\n",
      "Validation accuracy: 0.7869\n",
      "\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.87      0.76      2475\n",
      "           1       0.72      0.87      0.79      2464\n",
      "           2       0.93      0.71      0.80      5061\n",
      "\n",
      "    accuracy                           0.79     10000\n",
      "   macro avg       0.78      0.81      0.78     10000\n",
      "weighted avg       0.82      0.79      0.79     10000\n",
      "\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#@title Training\n",
    "# Cell 7: Training\n",
    "train_model(model, train_dataloader, test_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Test Set Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.84      0.76      2475\n",
      "           1       0.77      0.82      0.79      2464\n",
      "           2       0.91      0.79      0.85      5061\n",
      "\n",
      "    accuracy                           0.81     10000\n",
      "   macro avg       0.79      0.82      0.80     10000\n",
      "weighted avg       0.82      0.81      0.81     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#@title Test Evaluation\n",
    "# Cell 8: Final Evaluation\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load('best_splice_site_model.pt'))\n",
    "model.eval()\n",
    "\n",
    "# Evaluate on test set\n",
    "test_preds = []\n",
    "test_labels_all = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        _, logits = model(**batch)\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        test_preds.extend(predictions.cpu().numpy())\n",
    "        test_labels_all.extend(batch['labels'].cpu().numpy())\n",
    "\n",
    "print(\"\\nFinal Test Set Classification Report:\")\n",
    "print(classification_report(test_labels_all, test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
