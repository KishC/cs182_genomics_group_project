# %% [markdown]
# # GFF Prediction with DNABERT-2
# 
# This notebook demonstrates loading DNA sequences (FNA) and generating predictions using DNABERT-2

# %% [markdown]
# ## 1. Install Requirements

# %%
!pip install transformers
!pip install torch
!pip install biopython

# %% [markdown]
# ## 2. Import Libraries

# %%
import torch
from transformers import AutoTokenizer, AutoModel
from Bio import SeqIO
import numpy as np
import gzip

# %% [markdown]
# ## 3. Initialize DNABERT-2

# %%
MODEL_NAME = "zhihan1996/DNABERT-2-117M"

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
model = AutoModel.from_pretrained(MODEL_NAME, trust_remote_code=True)

# %% [markdown]
# ## 4. Sequence Preprocessing

# %%
def chunk_sequence(sequence, chunk_size=1000, overlap=200):
    """Split long sequences into overlapping chunks"""
    chunks = []
    for i in range(0, len(sequence), chunk_size - overlap):
        chunk = sequence[i:i + chunk_size]
        chunks.append(chunk)
        if i + chunk_size >= len(sequence):
            break
    return chunks

# %% [markdown]
# ## 5. Feature Prediction Function

# %%
def predict_features(sequence, model, tokenizer, max_length=512):
    # Preprocess sequence
    inputs = tokenizer(
        sequence,
        return_tensors="pt",
        padding="max_length",
        max_length=max_length,
        truncation=True
    )["input_ids"]
    
    # Get model outputs
    with torch.no_grad():
        outputs = model(inputs)
    
    # Example feature extraction (modify based on your model architecture)
    # This is a placeholder - you'll need to implement actual prediction logic
    features = []
    current_position = 0
    while current_position < len(sequence):
        end_position = min(current_position + 100, len(sequence))
        features.append({
            "type": "gene",
            "start": current_position + 1,  # GFF uses 1-based indexing
            "end": end_position,
            "score": 0.99,
            "strand": "+"
        })
        current_position = end_position
    
    return features

# %% [markdown]
# ## 6. Process FNA File

# %%
def process_fna(fna_path, model, tokenizer):
    # Read input sequence
    records = list(SeqIO.parse(fna_path, "fasta"))
    gff_output = []
    
    for record in records:
        sequence = str(record.seq)
        chunks = chunk_sequence(sequence)
        
        for chunk in chunks:
            features = predict_features(chunk, model, tokenizer)
            
            # Convert predictions to GFF format
            for feat in features:
                gff_line = (
                    f"{record.id}\tDNABERT2\t{feat['type']}\t"
                    f"{feat['start']}\t{feat['end']}\t{feat['score']}\t"
                    f"{feat['strand']}\t.\tID={record.id}_feature;"
                )
                gff_output.append(gff_line)
    
    return gff_output

# %% [markdown]
# ## 7. Example Usage

# %%
# Example FNA input
example_fna = """>seq1
ATGCGATCGATCGATCGATCGATCGATCGATCGATCGATCGATCG
"""

# Save example to file
with open("example.fna", "w") as f:
    f.write(example_fna)

# Process file
predictions = process_fna("example.fna", model, tokenizer)

# Display predictions
print("##gff-version 3")
for line in predictions:
    print(line)

# %% [markdown]
# ## Next Steps:
# 1. Implement custom model head for feature prediction
# 2. Add training loop with labeled data
# 3. Implement proper output decoding
# 4. Add evaluation metrics
# 5. Implement overlapping window merging
