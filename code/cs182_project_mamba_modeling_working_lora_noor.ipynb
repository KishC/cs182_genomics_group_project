{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip uninstall torch torchvision torchaudio transformers mamba-ssm tqdm pandas scikit-learn numpy -y\n",
    "# %pip cache purge\n",
    "# %pip install peft torch transformers mamba-ssm tqdm pandas scikit-learn numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>coord</th>\n",
       "      <th>kind</th>\n",
       "      <th>transcript</th>\n",
       "      <th>strand</th>\n",
       "      <th>chrom</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>sequence</th>\n",
       "      <th>win_start</th>\n",
       "      <th>win_end</th>\n",
       "      <th>is_truncated</th>\n",
       "      <th>motif_len</th>\n",
       "      <th>motif</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NC_050096.1_35318_donor</td>\n",
       "      <td>35318</td>\n",
       "      <td>0</td>\n",
       "      <td>XM_020544715.3</td>\n",
       "      <td>+</td>\n",
       "      <td>NC_050096.1</td>\n",
       "      <td>35319</td>\n",
       "      <td>35320</td>\n",
       "      <td>GGGCCCGGCTGGGCCTCAGCGGGGTCGTCGAGATGGAGATGGGGAG...</td>\n",
       "      <td>35118</td>\n",
       "      <td>35520</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>GT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NC_050096.1_34607_acceptor</td>\n",
       "      <td>34607</td>\n",
       "      <td>1</td>\n",
       "      <td>XM_020544715.3</td>\n",
       "      <td>+</td>\n",
       "      <td>NC_050096.1</td>\n",
       "      <td>34605</td>\n",
       "      <td>34606</td>\n",
       "      <td>TCCGGTGATTAATTTGTCCTTATACCTTTACAACAAAAATTCACTA...</td>\n",
       "      <td>34404</td>\n",
       "      <td>34806</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>TG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NC_050096.1_36174_donor</td>\n",
       "      <td>36174</td>\n",
       "      <td>0</td>\n",
       "      <td>XM_020544715.3</td>\n",
       "      <td>+</td>\n",
       "      <td>NC_050096.1</td>\n",
       "      <td>36175</td>\n",
       "      <td>36176</td>\n",
       "      <td>ATAATATGTTCATTATATCACAACACTCTTTTCTTATGGAGTCGTG...</td>\n",
       "      <td>35974</td>\n",
       "      <td>36376</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>GT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NC_050096.1_36037_acceptor</td>\n",
       "      <td>36037</td>\n",
       "      <td>1</td>\n",
       "      <td>XM_020544715.3</td>\n",
       "      <td>+</td>\n",
       "      <td>NC_050096.1</td>\n",
       "      <td>36035</td>\n",
       "      <td>36036</td>\n",
       "      <td>GCACAAAACTAACTAAAGGAATCATTCTGATAGATAACACTATAAA...</td>\n",
       "      <td>35834</td>\n",
       "      <td>36236</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>AG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NC_050096.1_36504_donor</td>\n",
       "      <td>36504</td>\n",
       "      <td>0</td>\n",
       "      <td>XM_020544715.3</td>\n",
       "      <td>+</td>\n",
       "      <td>NC_050096.1</td>\n",
       "      <td>36505</td>\n",
       "      <td>36506</td>\n",
       "      <td>TGTCATTTCCTTACCTCATTGAATCATTTCCGATGCTTCTTCTCTG...</td>\n",
       "      <td>36304</td>\n",
       "      <td>36706</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>GT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           id  coord  kind      transcript strand  \\\n",
       "0     NC_050096.1_35318_donor  35318     0  XM_020544715.3      +   \n",
       "1  NC_050096.1_34607_acceptor  34607     1  XM_020544715.3      +   \n",
       "2     NC_050096.1_36174_donor  36174     0  XM_020544715.3      +   \n",
       "3  NC_050096.1_36037_acceptor  36037     1  XM_020544715.3      +   \n",
       "4     NC_050096.1_36504_donor  36504     0  XM_020544715.3      +   \n",
       "\n",
       "         chrom  start    end  \\\n",
       "0  NC_050096.1  35319  35320   \n",
       "1  NC_050096.1  34605  34606   \n",
       "2  NC_050096.1  36175  36176   \n",
       "3  NC_050096.1  36035  36036   \n",
       "4  NC_050096.1  36505  36506   \n",
       "\n",
       "                                            sequence  win_start  win_end  \\\n",
       "0  GGGCCCGGCTGGGCCTCAGCGGGGTCGTCGAGATGGAGATGGGGAG...      35118    35520   \n",
       "1  TCCGGTGATTAATTTGTCCTTATACCTTTACAACAAAAATTCACTA...      34404    34806   \n",
       "2  ATAATATGTTCATTATATCACAACACTCTTTTCTTATGGAGTCGTG...      35974    36376   \n",
       "3  GCACAAAACTAACTAAAGGAATCATTCTGATAGATAACACTATAAA...      35834    36236   \n",
       "4  TGTCATTTCCTTACCTCATTGAATCATTTCCGATGCTTCTTCTCTG...      36304    36706   \n",
       "\n",
       "   is_truncated  motif_len motif  \n",
       "0         False          2    GT  \n",
       "1         False          2    TG  \n",
       "2         False          2    GT  \n",
       "3         False          2    AG  \n",
       "4         False          2    GT  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splice_df = pd.read_csv('splice_sites_full_centered_balanced_correct_V3.csv')\n",
    "splice_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data with stratification\n",
    "train_df, temp_df = train_test_split(splice_df, test_size=0.3, stratify=splice_df['kind'], random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['kind'], random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0+cu124\n",
      "CUDA available: True\n",
      "CUDA version: 12.4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from mamba_ssm import Mamba\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Dataset Class (Modified)\n",
    "class DNASequenceAndEmbeddingDataset(Dataset):\n",
    "    def __init__(self, sequences, labels, tokenizer, bert_model, device, max_length=512):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.bert_model = bert_model\n",
    "        self.device = device\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Nucleotide to index mapping\n",
    "        self.nuc_to_idx = {\n",
    "            'A': 0, 'C': 1, 'G': 2, 'T': 3, 'N': 4,\n",
    "            'P': 5  # P for padding\n",
    "        }\n",
    "        \n",
    "        # Pre-compute embeddings and encodings\n",
    "        self.embeddings = []\n",
    "        self.seq_encodings = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for seq in tqdm(sequences, desc=\"Computing embeddings\"):\n",
    "                # Get BERT embeddings\n",
    "                inputs = self.tokenizer(\n",
    "                    seq,\n",
    "                    padding=\"max_length\",\n",
    "                    max_length=max_length,\n",
    "                    truncation=True,\n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "                inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "                outputs = self.bert_model(**inputs)\n",
    "                # DNABERT-2 returns a tuple where the first element is hidden states\n",
    "                embeddings = outputs[0].cpu()  # Changed this line\n",
    "                self.embeddings.append(embeddings)\n",
    "                \n",
    "                # Create sequence encoding\n",
    "                seq_encoding = torch.tensor([\n",
    "                    self.nuc_to_idx.get(nuc, self.nuc_to_idx['N']) \n",
    "                    for nuc in (seq[:max_length] + 'P' * max(0, max_length - len(seq)))\n",
    "                ], dtype=torch.long)\n",
    "                self.seq_encodings.append(seq_encoding)\n",
    "        \n",
    "        # Convert labels to tensor\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'embeddings': self.embeddings[idx].squeeze(0),\n",
    "            'sequence': self.seq_encodings[idx],\n",
    "            'label': self.labels[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Model Architecture\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "class DNABertMambaWithSequenceClassifier(nn.Module):\n",
    "    def __init__(self, bert_model, d_model=768, n_classes=3, d_state=16, d_conv=4, expand=2, num_mamba_layers=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Apply LoRA to BERT model\n",
    "        \n",
    "        # Helper: Collect linear module names from DNABERT\n",
    "        def find_linear_layer_names(model):\n",
    "            target_names = set()\n",
    "            for name, module in model.named_modules():\n",
    "                if isinstance(module, torch.nn.Linear):\n",
    "                    # Only take the leaf module name\n",
    "                    if '.' in name:\n",
    "                        target_names.add(name.split('.')[-1])\n",
    "                    else:\n",
    "                        target_names.add(name)\n",
    "            return list(target_names)\n",
    "        \n",
    "        # Inside your model init\n",
    "        target_modules = find_linear_layer_names(bert_model)\n",
    "        print(\"found linear target modules:\", target_modules)\n",
    "        print(\"targeting linear target modules:\", [\"Wqkv\", \"dense\"])\n",
    "        \n",
    "        lora_config = LoraConfig(\n",
    "            r=8,\n",
    "            lora_alpha=32,\n",
    "            lora_dropout=0.1,\n",
    "            bias=\"none\",\n",
    "            task_type=TaskType.FEATURE_EXTRACTION,\n",
    "            target_modules=[\"Wqkv\", \"dense\"]\n",
    "            # target_modules=target_modules\n",
    "        )\n",
    "\n",
    "        self.bert_model = get_peft_model(bert_model, lora_config)\n",
    "        \n",
    "        # Nucleotide embedding\n",
    "        self.nuc_embedding = nn.Embedding(6, 32)  # 6 tokens (ACGTN + PAD), 32 dimensions\n",
    "        \n",
    "        # Combine sequence and BERT embeddings\n",
    "        self.combine_layer = nn.Sequential(\n",
    "            nn.LayerNorm(d_model + 32),  # Normalize *before* the Linear layer\n",
    "            nn.Linear(d_model + 32, d_model),\n",
    "            nn.GELU(), # I used GeLU instead of ReLU since was encountering model collapse\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "\n",
    "        # Mamba configuration with deeper layers\n",
    "        self.mamba = nn.Sequential(*[\n",
    "            Mamba(\n",
    "                d_model=d_model,\n",
    "                d_state=d_state,\n",
    "                d_conv=d_conv,\n",
    "                expand=expand,\n",
    "                use_fast_path=True\n",
    "            ) for _ in range(num_mamba_layers)\n",
    "        ])\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(d_model, n_classes)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "    def forward(self, embeddings, sequence, labels=None):\n",
    "        # Get nucleotide embeddings\n",
    "        seq_embeddings = self.nuc_embedding(sequence)\n",
    "        \n",
    "        # Combine BERT embeddings with sequence embeddings\n",
    "        combined = torch.cat([embeddings, seq_embeddings], dim=-1)\n",
    "        x = self.combine_layer(combined)\n",
    "        \n",
    "        # Pass through Mamba layers\n",
    "        x = self.mamba(x)\n",
    "        \n",
    "        # Global average pooling\n",
    "        x = x.mean(dim=1)\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(x)\n",
    "        \n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits, labels)\n",
    "            return loss, logits\n",
    "        return None, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Model Initialization (Modified)\n",
    "def prepare_model_and_data(train_df, test_df, sample_size=5000):\n",
    "    # Sample the data (keeping the stratified sampling)\n",
    "    train_sample_size = min(sample_size, len(train_df))\n",
    "    test_sample_size = min(sample_size//5, len(test_df))\n",
    "    \n",
    "    train_df_sample = train_df.groupby('kind', group_keys=False).apply(\n",
    "        lambda x: x.sample(n=min(len(x), train_sample_size // 3))\n",
    "    ).reset_index(drop=True)\n",
    "    \n",
    "    test_df_sample = test_df.groupby('kind', group_keys=False).apply(\n",
    "        lambda x: x.sample(n=min(len(x), test_sample_size // 3))\n",
    "    ).reset_index(drop=True)\n",
    "    \n",
    "    # Initialize BERT\n",
    "    model_name = \"zhihan1996/DNABERT-2-117M\"\n",
    "    \n",
    "    # Create config first\n",
    "    config = AutoConfig.from_pretrained(\n",
    "        model_name,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # Modify config to use standard attention\n",
    "    config.attention_probs_dropout_prob = 0.1\n",
    "    config.hidden_dropout_prob = 0.1\n",
    "    \n",
    "    # Initialize tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    \n",
    "    # Initialize BERT model\n",
    "    bert_model = AutoModel.from_pretrained(\n",
    "        model_name,\n",
    "        config=config,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # Force model to float32\n",
    "    bert_model = bert_model.float()\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Freeze BERT and move to device\n",
    "    for param in bert_model.parameters():\n",
    "        param.requires_grad = False\n",
    "    bert_model = bert_model.to(device)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = DNASequenceAndEmbeddingDataset(\n",
    "        sequences=train_df_sample['sequence'].tolist(),\n",
    "        labels=train_df_sample['kind'].tolist(),\n",
    "        tokenizer=tokenizer,\n",
    "        bert_model=bert_model,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    test_dataset = DNASequenceAndEmbeddingDataset(\n",
    "        sequences=test_df_sample['sequence'].tolist(),\n",
    "        labels=test_df_sample['kind'].tolist(),\n",
    "        tokenizer=tokenizer,\n",
    "        bert_model=bert_model,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Initialize model with LoRA and deeper Mamba layers\n",
    "    model = DNABertMambaWithSequenceClassifier(\n",
    "        bert_model=bert_model,\n",
    "        d_model=768,\n",
    "        n_classes=3,\n",
    "        d_state=16,\n",
    "        d_conv=4,\n",
    "        expand=2,\n",
    "        num_mamba_layers=4  # Increased number of layers\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    \n",
    "    return model, train_dataloader, test_dataloader, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not good\n",
    "def unfreeze_bert_layers(model, num_layers_to_unfreeze, max_layers=6):\n",
    "    encoder_layers = model.bert_model.base_model.encoder.layer\n",
    "    total_layers = len(encoder_layers)\n",
    "\n",
    "    # Enforce max cap\n",
    "    num_layers_to_unfreeze = min(num_layers_to_unfreeze, max_layers)\n",
    "\n",
    "    for i in range(total_layers - num_layers_to_unfreeze, total_layers):\n",
    "        for param in encoder_layers[i].parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    if hasattr(model.bert_model.base_model, 'pooler'):\n",
    "        for param in model.bert_model.base_model.pooler.parameters():\n",
    "            param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Training Function (Modified)\n",
    "def train_model(model, train_dataloader, val_dataloader, device, \n",
    "                num_epochs=10, patience=3, learning_rate=1e-4, \n",
    "                unfreeze_every_x_epoch=4, max_unfreeze_layers=6):\n",
    "    \n",
    "    # Initialize optimizer and tracking variables\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    # Initialize tracking lists\n",
    "    history = {\n",
    "        'epoch': [], 'train_loss': [], 'val_loss': [],\n",
    "        'train_acc': [], 'val_acc': [], 'learning_rate': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # if (epoch != 0) and (epoch % unfreeze_every_x_epoch == 0):\n",
    "        #     layers_to_unfreeze = epoch // unfreeze_every_x_epoch\n",
    "        #     print(f\"\\n[INFO] Unfreezing top {layers_to_unfreeze} BERT layers...\\n\")\n",
    "        #     unfreeze_bert_layers(model, layers_to_unfreeze, max_layers=max_unfreeze_layers)\n",
    "\n",
    "        trainable = [name for name, param in model.named_parameters() if param.requires_grad]\n",
    "        print(f\"[INFO] Trainable parameters after unfreezing: {len(trainable)}\")\n",
    "        for name in trainable:\n",
    "            print(f\" - {name}\")\n",
    "\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for batch in tqdm(train_dataloader, desc=f'Epoch {epoch + 1}/{num_epochs}'):\n",
    "            embeddings = batch['embeddings'].to(device)\n",
    "            sequence = batch['sequence'].to(device)\n",
    "            labels = batch['label'].to(device)  # Changed from 'labels' to 'label'\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss, logits = model(embeddings, sequence, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            train_correct += (predictions == labels).sum().item()\n",
    "            train_total += labels.size(0)\n",
    "        \n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "        train_accuracy = train_correct / train_total\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_dataloader:\n",
    "                embeddings = batch['embeddings'].to(device)\n",
    "                sequence = batch['sequence'].to(device)\n",
    "                labels = batch['label'].to(device)  # Changed from 'labels' to 'label'\n",
    "                \n",
    "                loss, logits = model(embeddings, sequence, labels)\n",
    "                total_val_loss += loss.item()\n",
    "                \n",
    "                # Calculate accuracy\n",
    "                predictions = torch.argmax(logits, dim=1)\n",
    "                val_correct += (predictions == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "        \n",
    "        avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "        val_accuracy = val_correct / val_total\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step(avg_val_loss)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # Save history\n",
    "        history['epoch'].append(epoch + 1)\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['train_acc'].append(train_accuracy)\n",
    "        history['val_acc'].append(val_accuracy)\n",
    "        history['learning_rate'].append(current_lr)\n",
    "        \n",
    "        # Print metrics\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}:')\n",
    "        print(f'Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.4f}')\n",
    "        print(f'Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.4f}')\n",
    "        print(f'Learning Rate: {current_lr}')\n",
    "        \n",
    "        # Early stopping check\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        if patience_counter >= patience:\n",
    "            print(f'Early stopping triggered after epoch {epoch + 1}')\n",
    "            break\n",
    "    \n",
    "    # Load best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    # Convert history to DataFrame\n",
    "    history_df = pd.DataFrame(history)\n",
    "    return history_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at zhihan1996/DNABERT-2-117M and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e764b66d05d44929b36f5c86449d5f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing embeddings:   0%|          | 0/4998 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c16a9c83c8e464a8f6f835388fe95da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing embeddings:   0%|          | 0/999 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found linear target modules: ['wo', 'gated_layers', 'Wqkv', 'dense']\n",
      "targeting linear target modules: ['Wqkv', 'dense']\n",
      "[INFO] Trainable parameters after unfreezing: 95\n",
      " - bert_model.base_model.model.encoder.layer.0.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.0.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.0.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.0.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.1.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.1.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.1.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.1.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.2.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.2.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.2.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.2.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.3.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.3.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.3.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.3.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.4.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.4.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.4.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.4.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.5.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.5.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.5.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.5.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.6.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.6.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.6.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.6.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.7.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.7.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.7.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.7.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.8.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.8.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.8.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.8.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.9.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.9.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.9.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.9.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.10.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.10.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.10.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.10.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.11.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.11.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.11.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.11.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.pooler.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.pooler.dense.lora_B.default.weight\n",
      " - nuc_embedding.weight\n",
      " - combine_layer.0.weight\n",
      " - combine_layer.0.bias\n",
      " - combine_layer.1.weight\n",
      " - combine_layer.1.bias\n",
      " - mamba.0.A_log\n",
      " - mamba.0.D\n",
      " - mamba.0.in_proj.weight\n",
      " - mamba.0.conv1d.weight\n",
      " - mamba.0.conv1d.bias\n",
      " - mamba.0.x_proj.weight\n",
      " - mamba.0.dt_proj.weight\n",
      " - mamba.0.dt_proj.bias\n",
      " - mamba.0.out_proj.weight\n",
      " - mamba.1.A_log\n",
      " - mamba.1.D\n",
      " - mamba.1.in_proj.weight\n",
      " - mamba.1.conv1d.weight\n",
      " - mamba.1.conv1d.bias\n",
      " - mamba.1.x_proj.weight\n",
      " - mamba.1.dt_proj.weight\n",
      " - mamba.1.dt_proj.bias\n",
      " - mamba.1.out_proj.weight\n",
      " - mamba.2.A_log\n",
      " - mamba.2.D\n",
      " - mamba.2.in_proj.weight\n",
      " - mamba.2.conv1d.weight\n",
      " - mamba.2.conv1d.bias\n",
      " - mamba.2.x_proj.weight\n",
      " - mamba.2.dt_proj.weight\n",
      " - mamba.2.dt_proj.bias\n",
      " - mamba.2.out_proj.weight\n",
      " - mamba.3.A_log\n",
      " - mamba.3.D\n",
      " - mamba.3.in_proj.weight\n",
      " - mamba.3.conv1d.weight\n",
      " - mamba.3.conv1d.bias\n",
      " - mamba.3.x_proj.weight\n",
      " - mamba.3.dt_proj.weight\n",
      " - mamba.3.dt_proj.bias\n",
      " - mamba.3.out_proj.weight\n",
      " - classifier.0.weight\n",
      " - classifier.0.bias\n",
      " - classifier.2.weight\n",
      " - classifier.2.bias\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a77c48694e4a4f18af5b0cf753741132",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/15:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15:\n",
      "Train Loss: 1.0858, Train Acc: 0.3585\n",
      "Val Loss: 0.9810, Val Acc: 0.4925\n",
      "Learning Rate: 0.0001\n",
      "[INFO] Trainable parameters after unfreezing: 95\n",
      " - bert_model.base_model.model.encoder.layer.0.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.0.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.0.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.0.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.1.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.1.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.1.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.1.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.2.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.2.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.2.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.2.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.3.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.3.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.3.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.3.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.4.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.4.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.4.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.4.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.5.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.5.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.5.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.5.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.6.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.6.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.6.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.6.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.7.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.7.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.7.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.7.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.8.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.8.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.8.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.8.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.9.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.9.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.9.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.9.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.10.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.10.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.10.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.10.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.11.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.11.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.11.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.11.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.pooler.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.pooler.dense.lora_B.default.weight\n",
      " - nuc_embedding.weight\n",
      " - combine_layer.0.weight\n",
      " - combine_layer.0.bias\n",
      " - combine_layer.1.weight\n",
      " - combine_layer.1.bias\n",
      " - mamba.0.A_log\n",
      " - mamba.0.D\n",
      " - mamba.0.in_proj.weight\n",
      " - mamba.0.conv1d.weight\n",
      " - mamba.0.conv1d.bias\n",
      " - mamba.0.x_proj.weight\n",
      " - mamba.0.dt_proj.weight\n",
      " - mamba.0.dt_proj.bias\n",
      " - mamba.0.out_proj.weight\n",
      " - mamba.1.A_log\n",
      " - mamba.1.D\n",
      " - mamba.1.in_proj.weight\n",
      " - mamba.1.conv1d.weight\n",
      " - mamba.1.conv1d.bias\n",
      " - mamba.1.x_proj.weight\n",
      " - mamba.1.dt_proj.weight\n",
      " - mamba.1.dt_proj.bias\n",
      " - mamba.1.out_proj.weight\n",
      " - mamba.2.A_log\n",
      " - mamba.2.D\n",
      " - mamba.2.in_proj.weight\n",
      " - mamba.2.conv1d.weight\n",
      " - mamba.2.conv1d.bias\n",
      " - mamba.2.x_proj.weight\n",
      " - mamba.2.dt_proj.weight\n",
      " - mamba.2.dt_proj.bias\n",
      " - mamba.2.out_proj.weight\n",
      " - mamba.3.A_log\n",
      " - mamba.3.D\n",
      " - mamba.3.in_proj.weight\n",
      " - mamba.3.conv1d.weight\n",
      " - mamba.3.conv1d.bias\n",
      " - mamba.3.x_proj.weight\n",
      " - mamba.3.dt_proj.weight\n",
      " - mamba.3.dt_proj.bias\n",
      " - mamba.3.out_proj.weight\n",
      " - classifier.0.weight\n",
      " - classifier.0.bias\n",
      " - classifier.2.weight\n",
      " - classifier.2.bias\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98c6cf63450244caaa7206622e85bd67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/15:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/15:\n",
      "Train Loss: 0.9423, Train Acc: 0.4932\n",
      "Val Loss: 0.8749, Val Acc: 0.5315\n",
      "Learning Rate: 0.0001\n",
      "[INFO] Trainable parameters after unfreezing: 95\n",
      " - bert_model.base_model.model.encoder.layer.0.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.0.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.0.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.0.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.1.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.1.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.1.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.1.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.2.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.2.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.2.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.2.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.3.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.3.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.3.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.3.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.4.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.4.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.4.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.4.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.5.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.5.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.5.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.5.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.6.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.6.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.6.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.6.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.7.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.7.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.7.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.7.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.8.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.8.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.8.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.8.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.9.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.9.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.9.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.9.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.10.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.10.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.10.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.10.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.11.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.11.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.11.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.11.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.pooler.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.pooler.dense.lora_B.default.weight\n",
      " - nuc_embedding.weight\n",
      " - combine_layer.0.weight\n",
      " - combine_layer.0.bias\n",
      " - combine_layer.1.weight\n",
      " - combine_layer.1.bias\n",
      " - mamba.0.A_log\n",
      " - mamba.0.D\n",
      " - mamba.0.in_proj.weight\n",
      " - mamba.0.conv1d.weight\n",
      " - mamba.0.conv1d.bias\n",
      " - mamba.0.x_proj.weight\n",
      " - mamba.0.dt_proj.weight\n",
      " - mamba.0.dt_proj.bias\n",
      " - mamba.0.out_proj.weight\n",
      " - mamba.1.A_log\n",
      " - mamba.1.D\n",
      " - mamba.1.in_proj.weight\n",
      " - mamba.1.conv1d.weight\n",
      " - mamba.1.conv1d.bias\n",
      " - mamba.1.x_proj.weight\n",
      " - mamba.1.dt_proj.weight\n",
      " - mamba.1.dt_proj.bias\n",
      " - mamba.1.out_proj.weight\n",
      " - mamba.2.A_log\n",
      " - mamba.2.D\n",
      " - mamba.2.in_proj.weight\n",
      " - mamba.2.conv1d.weight\n",
      " - mamba.2.conv1d.bias\n",
      " - mamba.2.x_proj.weight\n",
      " - mamba.2.dt_proj.weight\n",
      " - mamba.2.dt_proj.bias\n",
      " - mamba.2.out_proj.weight\n",
      " - mamba.3.A_log\n",
      " - mamba.3.D\n",
      " - mamba.3.in_proj.weight\n",
      " - mamba.3.conv1d.weight\n",
      " - mamba.3.conv1d.bias\n",
      " - mamba.3.x_proj.weight\n",
      " - mamba.3.dt_proj.weight\n",
      " - mamba.3.dt_proj.bias\n",
      " - mamba.3.out_proj.weight\n",
      " - classifier.0.weight\n",
      " - classifier.0.bias\n",
      " - classifier.2.weight\n",
      " - classifier.2.bias\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bcfaf015720402a8d3b4fe175308266",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/15:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/15:\n",
      "Train Loss: 0.8542, Train Acc: 0.5400\n",
      "Val Loss: 0.8980, Val Acc: 0.5055\n",
      "Learning Rate: 0.0001\n",
      "[INFO] Trainable parameters after unfreezing: 95\n",
      " - bert_model.base_model.model.encoder.layer.0.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.0.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.0.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.0.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.1.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.1.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.1.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.1.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.2.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.2.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.2.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.2.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.3.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.3.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.3.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.3.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.4.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.4.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.4.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.4.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.5.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.5.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.5.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.5.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.6.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.6.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.6.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.6.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.7.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.7.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.7.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.7.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.8.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.8.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.8.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.8.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.9.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.9.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.9.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.9.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.10.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.10.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.10.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.10.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.11.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.11.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.11.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.11.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.pooler.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.pooler.dense.lora_B.default.weight\n",
      " - nuc_embedding.weight\n",
      " - combine_layer.0.weight\n",
      " - combine_layer.0.bias\n",
      " - combine_layer.1.weight\n",
      " - combine_layer.1.bias\n",
      " - mamba.0.A_log\n",
      " - mamba.0.D\n",
      " - mamba.0.in_proj.weight\n",
      " - mamba.0.conv1d.weight\n",
      " - mamba.0.conv1d.bias\n",
      " - mamba.0.x_proj.weight\n",
      " - mamba.0.dt_proj.weight\n",
      " - mamba.0.dt_proj.bias\n",
      " - mamba.0.out_proj.weight\n",
      " - mamba.1.A_log\n",
      " - mamba.1.D\n",
      " - mamba.1.in_proj.weight\n",
      " - mamba.1.conv1d.weight\n",
      " - mamba.1.conv1d.bias\n",
      " - mamba.1.x_proj.weight\n",
      " - mamba.1.dt_proj.weight\n",
      " - mamba.1.dt_proj.bias\n",
      " - mamba.1.out_proj.weight\n",
      " - mamba.2.A_log\n",
      " - mamba.2.D\n",
      " - mamba.2.in_proj.weight\n",
      " - mamba.2.conv1d.weight\n",
      " - mamba.2.conv1d.bias\n",
      " - mamba.2.x_proj.weight\n",
      " - mamba.2.dt_proj.weight\n",
      " - mamba.2.dt_proj.bias\n",
      " - mamba.2.out_proj.weight\n",
      " - mamba.3.A_log\n",
      " - mamba.3.D\n",
      " - mamba.3.in_proj.weight\n",
      " - mamba.3.conv1d.weight\n",
      " - mamba.3.conv1d.bias\n",
      " - mamba.3.x_proj.weight\n",
      " - mamba.3.dt_proj.weight\n",
      " - mamba.3.dt_proj.bias\n",
      " - mamba.3.out_proj.weight\n",
      " - classifier.0.weight\n",
      " - classifier.0.bias\n",
      " - classifier.2.weight\n",
      " - classifier.2.bias\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a20266eb0070437a92d62633eeb23a42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/15:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/15:\n",
      "Train Loss: 0.8289, Train Acc: 0.5482\n",
      "Val Loss: 0.8464, Val Acc: 0.5405\n",
      "Learning Rate: 0.0001\n",
      "[INFO] Trainable parameters after unfreezing: 95\n",
      " - bert_model.base_model.model.encoder.layer.0.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.0.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.0.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.0.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.1.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.1.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.1.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.1.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.2.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.2.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.2.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.2.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.3.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.3.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.3.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.3.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.4.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.4.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.4.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.4.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.5.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.5.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.5.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.5.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.6.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.6.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.6.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.6.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.7.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.7.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.7.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.7.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.8.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.8.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.8.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.8.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.9.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.9.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.9.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.9.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.10.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.10.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.10.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.10.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.11.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.11.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.11.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.11.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.pooler.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.pooler.dense.lora_B.default.weight\n",
      " - nuc_embedding.weight\n",
      " - combine_layer.0.weight\n",
      " - combine_layer.0.bias\n",
      " - combine_layer.1.weight\n",
      " - combine_layer.1.bias\n",
      " - mamba.0.A_log\n",
      " - mamba.0.D\n",
      " - mamba.0.in_proj.weight\n",
      " - mamba.0.conv1d.weight\n",
      " - mamba.0.conv1d.bias\n",
      " - mamba.0.x_proj.weight\n",
      " - mamba.0.dt_proj.weight\n",
      " - mamba.0.dt_proj.bias\n",
      " - mamba.0.out_proj.weight\n",
      " - mamba.1.A_log\n",
      " - mamba.1.D\n",
      " - mamba.1.in_proj.weight\n",
      " - mamba.1.conv1d.weight\n",
      " - mamba.1.conv1d.bias\n",
      " - mamba.1.x_proj.weight\n",
      " - mamba.1.dt_proj.weight\n",
      " - mamba.1.dt_proj.bias\n",
      " - mamba.1.out_proj.weight\n",
      " - mamba.2.A_log\n",
      " - mamba.2.D\n",
      " - mamba.2.in_proj.weight\n",
      " - mamba.2.conv1d.weight\n",
      " - mamba.2.conv1d.bias\n",
      " - mamba.2.x_proj.weight\n",
      " - mamba.2.dt_proj.weight\n",
      " - mamba.2.dt_proj.bias\n",
      " - mamba.2.out_proj.weight\n",
      " - mamba.3.A_log\n",
      " - mamba.3.D\n",
      " - mamba.3.in_proj.weight\n",
      " - mamba.3.conv1d.weight\n",
      " - mamba.3.conv1d.bias\n",
      " - mamba.3.x_proj.weight\n",
      " - mamba.3.dt_proj.weight\n",
      " - mamba.3.dt_proj.bias\n",
      " - mamba.3.out_proj.weight\n",
      " - classifier.0.weight\n",
      " - classifier.0.bias\n",
      " - classifier.2.weight\n",
      " - classifier.2.bias\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04ab5612650d4b429d29c15686d129ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/15:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/15:\n",
      "Train Loss: 0.7869, Train Acc: 0.5764\n",
      "Val Loss: 0.8030, Val Acc: 0.5656\n",
      "Learning Rate: 0.0001\n",
      "[INFO] Trainable parameters after unfreezing: 95\n",
      " - bert_model.base_model.model.encoder.layer.0.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.0.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.0.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.0.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.1.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.1.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.1.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.1.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.2.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.2.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.2.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.2.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.3.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.3.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.3.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.3.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.4.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.4.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.4.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.4.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.5.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.5.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.5.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.5.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.6.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.6.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.6.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.6.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.7.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.7.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.7.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.7.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.8.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.8.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.8.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.8.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.9.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.9.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.9.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.9.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.10.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.10.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.10.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.10.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.11.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.11.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.11.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.11.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.pooler.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.pooler.dense.lora_B.default.weight\n",
      " - nuc_embedding.weight\n",
      " - combine_layer.0.weight\n",
      " - combine_layer.0.bias\n",
      " - combine_layer.1.weight\n",
      " - combine_layer.1.bias\n",
      " - mamba.0.A_log\n",
      " - mamba.0.D\n",
      " - mamba.0.in_proj.weight\n",
      " - mamba.0.conv1d.weight\n",
      " - mamba.0.conv1d.bias\n",
      " - mamba.0.x_proj.weight\n",
      " - mamba.0.dt_proj.weight\n",
      " - mamba.0.dt_proj.bias\n",
      " - mamba.0.out_proj.weight\n",
      " - mamba.1.A_log\n",
      " - mamba.1.D\n",
      " - mamba.1.in_proj.weight\n",
      " - mamba.1.conv1d.weight\n",
      " - mamba.1.conv1d.bias\n",
      " - mamba.1.x_proj.weight\n",
      " - mamba.1.dt_proj.weight\n",
      " - mamba.1.dt_proj.bias\n",
      " - mamba.1.out_proj.weight\n",
      " - mamba.2.A_log\n",
      " - mamba.2.D\n",
      " - mamba.2.in_proj.weight\n",
      " - mamba.2.conv1d.weight\n",
      " - mamba.2.conv1d.bias\n",
      " - mamba.2.x_proj.weight\n",
      " - mamba.2.dt_proj.weight\n",
      " - mamba.2.dt_proj.bias\n",
      " - mamba.2.out_proj.weight\n",
      " - mamba.3.A_log\n",
      " - mamba.3.D\n",
      " - mamba.3.in_proj.weight\n",
      " - mamba.3.conv1d.weight\n",
      " - mamba.3.conv1d.bias\n",
      " - mamba.3.x_proj.weight\n",
      " - mamba.3.dt_proj.weight\n",
      " - mamba.3.dt_proj.bias\n",
      " - mamba.3.out_proj.weight\n",
      " - classifier.0.weight\n",
      " - classifier.0.bias\n",
      " - classifier.2.weight\n",
      " - classifier.2.bias\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d907245640d145758b4d9303e35baade",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/15:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/15:\n",
      "Train Loss: 0.7565, Train Acc: 0.5774\n",
      "Val Loss: 0.8884, Val Acc: 0.5095\n",
      "Learning Rate: 0.0001\n",
      "[INFO] Trainable parameters after unfreezing: 95\n",
      " - bert_model.base_model.model.encoder.layer.0.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.0.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.0.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.0.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.1.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.1.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.1.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.1.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.2.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.2.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.2.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.2.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.3.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.3.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.3.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.3.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.4.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.4.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.4.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.4.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.5.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.5.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.5.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.5.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.6.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.6.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.6.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.6.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.7.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.7.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.7.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.7.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.8.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.8.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.8.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.8.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.9.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.9.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.9.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.9.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.10.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.10.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.10.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.10.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.11.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.11.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.11.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.11.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.pooler.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.pooler.dense.lora_B.default.weight\n",
      " - nuc_embedding.weight\n",
      " - combine_layer.0.weight\n",
      " - combine_layer.0.bias\n",
      " - combine_layer.1.weight\n",
      " - combine_layer.1.bias\n",
      " - mamba.0.A_log\n",
      " - mamba.0.D\n",
      " - mamba.0.in_proj.weight\n",
      " - mamba.0.conv1d.weight\n",
      " - mamba.0.conv1d.bias\n",
      " - mamba.0.x_proj.weight\n",
      " - mamba.0.dt_proj.weight\n",
      " - mamba.0.dt_proj.bias\n",
      " - mamba.0.out_proj.weight\n",
      " - mamba.1.A_log\n",
      " - mamba.1.D\n",
      " - mamba.1.in_proj.weight\n",
      " - mamba.1.conv1d.weight\n",
      " - mamba.1.conv1d.bias\n",
      " - mamba.1.x_proj.weight\n",
      " - mamba.1.dt_proj.weight\n",
      " - mamba.1.dt_proj.bias\n",
      " - mamba.1.out_proj.weight\n",
      " - mamba.2.A_log\n",
      " - mamba.2.D\n",
      " - mamba.2.in_proj.weight\n",
      " - mamba.2.conv1d.weight\n",
      " - mamba.2.conv1d.bias\n",
      " - mamba.2.x_proj.weight\n",
      " - mamba.2.dt_proj.weight\n",
      " - mamba.2.dt_proj.bias\n",
      " - mamba.2.out_proj.weight\n",
      " - mamba.3.A_log\n",
      " - mamba.3.D\n",
      " - mamba.3.in_proj.weight\n",
      " - mamba.3.conv1d.weight\n",
      " - mamba.3.conv1d.bias\n",
      " - mamba.3.x_proj.weight\n",
      " - mamba.3.dt_proj.weight\n",
      " - mamba.3.dt_proj.bias\n",
      " - mamba.3.out_proj.weight\n",
      " - classifier.0.weight\n",
      " - classifier.0.bias\n",
      " - classifier.2.weight\n",
      " - classifier.2.bias\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "640679df28734c1e9886dca11a46aa35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/15:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/15:\n",
      "Train Loss: 0.7405, Train Acc: 0.5956\n",
      "Val Loss: 0.9520, Val Acc: 0.5365\n",
      "Learning Rate: 0.0001\n",
      "[INFO] Trainable parameters after unfreezing: 95\n",
      " - bert_model.base_model.model.encoder.layer.0.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.0.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.0.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.0.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.1.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.1.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.1.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.1.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.2.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.2.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.2.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.2.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.3.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.3.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.3.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.3.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.4.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.4.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.4.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.4.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.5.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.5.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.5.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.5.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.6.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.6.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.6.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.6.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.7.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.7.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.7.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.7.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.8.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.8.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.8.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.8.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.9.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.9.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.9.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.9.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.10.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.10.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.10.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.10.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.11.attention.self.Wqkv.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.11.attention.self.Wqkv.lora_B.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.11.attention.output.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.encoder.layer.11.attention.output.dense.lora_B.default.weight\n",
      " - bert_model.base_model.model.pooler.dense.lora_A.default.weight\n",
      " - bert_model.base_model.model.pooler.dense.lora_B.default.weight\n",
      " - nuc_embedding.weight\n",
      " - combine_layer.0.weight\n",
      " - combine_layer.0.bias\n",
      " - combine_layer.1.weight\n",
      " - combine_layer.1.bias\n",
      " - mamba.0.A_log\n",
      " - mamba.0.D\n",
      " - mamba.0.in_proj.weight\n",
      " - mamba.0.conv1d.weight\n",
      " - mamba.0.conv1d.bias\n",
      " - mamba.0.x_proj.weight\n",
      " - mamba.0.dt_proj.weight\n",
      " - mamba.0.dt_proj.bias\n",
      " - mamba.0.out_proj.weight\n",
      " - mamba.1.A_log\n",
      " - mamba.1.D\n",
      " - mamba.1.in_proj.weight\n",
      " - mamba.1.conv1d.weight\n",
      " - mamba.1.conv1d.bias\n",
      " - mamba.1.x_proj.weight\n",
      " - mamba.1.dt_proj.weight\n",
      " - mamba.1.dt_proj.bias\n",
      " - mamba.1.out_proj.weight\n",
      " - mamba.2.A_log\n",
      " - mamba.2.D\n",
      " - mamba.2.in_proj.weight\n",
      " - mamba.2.conv1d.weight\n",
      " - mamba.2.conv1d.bias\n",
      " - mamba.2.x_proj.weight\n",
      " - mamba.2.dt_proj.weight\n",
      " - mamba.2.dt_proj.bias\n",
      " - mamba.2.out_proj.weight\n",
      " - mamba.3.A_log\n",
      " - mamba.3.D\n",
      " - mamba.3.in_proj.weight\n",
      " - mamba.3.conv1d.weight\n",
      " - mamba.3.conv1d.bias\n",
      " - mamba.3.x_proj.weight\n",
      " - mamba.3.dt_proj.weight\n",
      " - mamba.3.dt_proj.bias\n",
      " - mamba.3.out_proj.weight\n",
      " - classifier.0.weight\n",
      " - classifier.0.bias\n",
      " - classifier.2.weight\n",
      " - classifier.2.bias\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57b63c5fedc44cd09c11a7fb1095d11a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/15:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/15:\n",
      "Train Loss: 0.6193, Train Acc: 0.6371\n",
      "Val Loss: 1.0802, Val Acc: 0.5265\n",
      "Learning Rate: 1e-05\n",
      "Early stopping triggered after epoch 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Cell 6: Run the Pipeline\n",
    "# Initialize model and data\n",
    "model, train_dataloader, test_dataloader, device = prepare_model_and_data(\n",
    "    train_df=train_df,\n",
    "    test_df=test_df,\n",
    "    sample_size=5000\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history_df = train_model(\n",
    "    model=model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloader=test_dataloader,\n",
    "    device=device,\n",
    "    num_epochs=15,\n",
    "    patience=3,\n",
    "    learning_rate=1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_df['train_loss'], label='Train Loss')\n",
    "plt.plot(history_df['val_loss'], label='Val Loss')\n",
    "plt.title('Loss Over Time')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_df['train_acc'], label='Train Acc')\n",
    "plt.plot(history_df['val_acc'], label='Val Acc')\n",
    "plt.title('Accuracy Over Time')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Compute Test Accuracy\n",
    "def compute_test_accuracy(model, test_dataloader, device):\n",
    "    model.eval()\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            embeddings = batch['embeddings'].to(device)\n",
    "            sequence = batch['sequence'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            _, logits = model(embeddings, sequence)\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            test_correct += (predictions == labels).sum().item()\n",
    "            test_total += labels.size(0)\n",
    "    \n",
    "    test_accuracy = test_correct / test_total\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Compute test accuracy\n",
    "compute_test_accuracy(model, test_dataloader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NERSC Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
