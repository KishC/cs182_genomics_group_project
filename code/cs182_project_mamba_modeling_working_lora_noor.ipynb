{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip uninstall torch torchvision torchaudio transformers mamba-ssm tqdm pandas scikit-learn numpy -y\n",
    "# %pip cache purge\n",
    "# %pip install peft torch transformers mamba-ssm tqdm pandas scikit-learn numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splice_df = pd.read_csv('splice_sites_full_centered_balanced_correct_V3.csv')\n",
    "splice_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data with stratification\n",
    "train_df, temp_df = train_test_split(splice_df, test_size=0.3, stratify=splice_df['kind'], random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['kind'], random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from mamba_ssm import Mamba\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Dataset Class (Modified)\n",
    "class DNASequenceAndEmbeddingDataset(Dataset):\n",
    "    def __init__(self, sequences, labels, tokenizer, bert_model, device, max_length=512):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.bert_model = bert_model\n",
    "        self.device = device\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Nucleotide to index mapping\n",
    "        self.nuc_to_idx = {\n",
    "            'A': 0, 'C': 1, 'G': 2, 'T': 3, 'N': 4,\n",
    "            'P': 5  # P for padding\n",
    "        }\n",
    "        \n",
    "        # Pre-compute embeddings and encodings\n",
    "        # Pre-compute embeddings and encodings\n",
    "        self.embeddings = []\n",
    "        self.seq_encodings = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for seq in tqdm(sequences, desc=\"Computing embeddings\"):\n",
    "                # Get BERT embeddings\n",
    "                inputs = self.tokenizer(\n",
    "                    seq,\n",
    "                    padding=\"max_length\",\n",
    "                    max_length=max_length,\n",
    "                    truncation=True,\n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "                inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "                outputs = self.bert_model(**inputs)\n",
    "                # DNABERT-2 returns a tuple where the first element is hidden states\n",
    "                embeddings = outputs[0].cpu()  # Changed this line\n",
    "                self.embeddings.append(embeddings)\n",
    "                \n",
    "                # Create sequence encoding\n",
    "                seq_encoding = torch.tensor([\n",
    "                    self.nuc_to_idx.get(nuc, self.nuc_to_idx['N']) \n",
    "                    for nuc in (seq[:max_length] + 'P' * max(0, max_length - len(seq)))\n",
    "                ], dtype=torch.long)\n",
    "                self.seq_encodings.append(seq_encoding)\n",
    "        \n",
    "        # Convert labels to tensor\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'embeddings': self.embeddings[idx].squeeze(0),\n",
    "            'sequence': self.seq_encodings[idx],\n",
    "            'label': self.labels[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Model Architecture\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "class DNABertMambaWithSequenceClassifier(nn.Module):\n",
    "    def __init__(self, bert_model, d_model=768, n_classes=3, d_state=16, d_conv=4, expand=2, num_mamba_layers=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Apply LoRA to BERT model\n",
    "        \n",
    "        # Helper: Collect linear module names from DNABERT\n",
    "        def find_linear_layer_names(model):\n",
    "            target_names = set()\n",
    "            for name, module in model.named_modules():\n",
    "                if isinstance(module, torch.nn.Linear):\n",
    "                    # Only take the leaf module name\n",
    "                    if '.' in name:\n",
    "                        target_names.add(name.split('.')[-1])\n",
    "                    else:\n",
    "                        target_names.add(name)\n",
    "            return list(target_names)\n",
    "        \n",
    "        # Inside your model init\n",
    "        target_modules = find_linear_layer_names(bert_model)\n",
    "        print(\"found linear target modules:\", target_modules)\n",
    "        print(\"targeting linear target modules:\", [\"Wqkv\", \"dense\"])\n",
    "        \n",
    "        lora_config = LoraConfig(\n",
    "            r=8,\n",
    "            lora_alpha=32,\n",
    "            lora_dropout=0.1,\n",
    "            bias=\"none\",\n",
    "            task_type=TaskType.FEATURE_EXTRACTION,\n",
    "            target_modules=[\"Wqkv\", \"dense\"]\n",
    "            # target_modules=target_modules\n",
    "        )\n",
    "\n",
    "        self.bert_model = get_peft_model(bert_model, lora_config)\n",
    "        \n",
    "        # Nucleotide embedding\n",
    "        self.nuc_embedding = nn.Embedding(6, 32)  # 6 tokens (ACGTN + PAD), 32 dimensions\n",
    "        \n",
    "        # Combine sequence and BERT embeddings\n",
    "        self.combine_layer = nn.Sequential(\n",
    "            nn.LayerNorm(d_model + 32),  # Normalize *before* the Linear layer\n",
    "            nn.Linear(d_model + 32, d_model),\n",
    "            nn.GELU(), # I used GeLU instead of ReLU since was encountering model collapse\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "\n",
    "        # Mamba configuration with deeper layers\n",
    "        self.mamba = nn.Sequential(*[\n",
    "            Mamba(\n",
    "                d_model=d_model,\n",
    "                d_state=d_state,\n",
    "                d_conv=d_conv,\n",
    "                expand=expand,\n",
    "                use_fast_path=True\n",
    "            ) for _ in range(num_mamba_layers)\n",
    "        ])\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(d_model, n_classes)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "    def forward(self, embeddings, sequence, labels=None):\n",
    "        # Get nucleotide embeddings\n",
    "        seq_embeddings = self.nuc_embedding(sequence)\n",
    "        \n",
    "        # Combine BERT embeddings with sequence embeddings\n",
    "        combined = torch.cat([embeddings, seq_embeddings], dim=-1)\n",
    "        x = self.combine_layer(combined)\n",
    "        \n",
    "        # Pass through Mamba layers after clamp\n",
    "        x = torch.clamp(x, -10.0, 10.0)\n",
    "        \n",
    "        x = self.mamba(x)\n",
    "        \n",
    "        # Global average pooling\n",
    "        x = x.mean(dim=1)\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(x)\n",
    "        \n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits, labels)\n",
    "            return loss, logits\n",
    "        return None, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Model Initialization (Modified)\n",
    "def prepare_model_and_data(train_df, test_df, sample_size=5000):\n",
    "    # Sample the data (keeping the stratified sampling)\n",
    "    train_sample_size = min(sample_size, len(train_df))\n",
    "    test_sample_size = min(sample_size//5, len(test_df))\n",
    "    \n",
    "    train_df_sample = train_df.groupby('kind', group_keys=False).apply(\n",
    "        lambda x: x.sample(n=min(len(x), train_sample_size // 3))\n",
    "    ).reset_index(drop=True)\n",
    "    \n",
    "    test_df_sample = test_df.groupby('kind', group_keys=False).apply(\n",
    "        lambda x: x.sample(n=min(len(x), test_sample_size // 3))\n",
    "    ).reset_index(drop=True)\n",
    "    \n",
    "    # Initialize BERT\n",
    "    model_name = \"zhihan1996/DNABERT-2-117M\"\n",
    "    \n",
    "    # Create config first\n",
    "    config = AutoConfig.from_pretrained(\n",
    "        model_name,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # Modify config to use standard attention\n",
    "    config.attention_probs_dropout_prob = 0.1\n",
    "    config.hidden_dropout_prob = 0.1\n",
    "    \n",
    "    # Initialize tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    \n",
    "    # Initialize BERT model\n",
    "    bert_model = AutoModel.from_pretrained(\n",
    "        model_name,\n",
    "        config=config,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # Force model to float32\n",
    "    bert_model = bert_model.float()\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Freeze BERT and move to device\n",
    "    for param in bert_model.parameters():\n",
    "        param.requires_grad = False\n",
    "    bert_model = bert_model.to(device)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = DNASequenceAndEmbeddingDataset(\n",
    "        sequences=train_df_sample['sequence'].tolist(),\n",
    "        labels=train_df_sample['kind'].tolist(),\n",
    "        tokenizer=tokenizer,\n",
    "        bert_model=bert_model,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    test_dataset = DNASequenceAndEmbeddingDataset(\n",
    "        sequences=test_df_sample['sequence'].tolist(),\n",
    "        labels=test_df_sample['kind'].tolist(),\n",
    "        tokenizer=tokenizer,\n",
    "        bert_model=bert_model,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Initialize model with LoRA and deeper Mamba layers\n",
    "    model = DNABertMambaWithSequenceClassifier(\n",
    "        bert_model=bert_model,\n",
    "        d_model=768,\n",
    "        n_classes=3,\n",
    "        d_state=16,\n",
    "        d_conv=4,\n",
    "        expand=2,\n",
    "        num_mamba_layers=4  # Increased number of layers\n",
    "    )\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Sanity check for DNABERT tokenizer\n",
    "    example_seq = train_df_sample['sequence'].iloc[0]\n",
    "    print(\"Original sequence:\", example_seq)\n",
    "    \n",
    "    # Check tokens\n",
    "    tokens = tokenizer.tokenize(example_seq)\n",
    "    print(\"Tokenized:\", tokens)\n",
    "    \n",
    "    # Check input ids\n",
    "    inputs = tokenizer(example_seq, return_tensors=\"pt\", padding=\"max_length\", max_length=512, truncation=True)\n",
    "    print(\"Input IDs shape:\", inputs['input_ids'].shape)\n",
    "    print(\"First few IDs:\", inputs['input_ids'][0, :10])\n",
    "    \n",
    "    return model, train_dataloader, test_dataloader, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not good\n",
    "def unfreeze_bert_layers(model, num_layers_to_unfreeze, max_layers=6):\n",
    "    encoder_layers = model.bert_model.base_model.encoder.layer\n",
    "    total_layers = len(encoder_layers)\n",
    "\n",
    "    # Enforce max cap\n",
    "    num_layers_to_unfreeze = min(num_layers_to_unfreeze, max_layers)\n",
    "\n",
    "    for i in range(total_layers - num_layers_to_unfreeze, total_layers):\n",
    "        for param in encoder_layers[i].parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    if hasattr(model.bert_model.base_model, 'pooler'):\n",
    "        for param in model.bert_model.base_model.pooler.parameters():\n",
    "            param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Training Function (Modified)\n",
    "def train_model(model, train_dataloader, val_dataloader, device, \n",
    "                num_epochs=10, patience=3, learning_rate=1e-4, \n",
    "                unfreeze_every_x_epoch=0, max_unfreeze_layers=0):\n",
    "    \n",
    "    # Initialize optimizer and tracking variables\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    # Initialize tracking lists\n",
    "    history = {\n",
    "        'epoch': [], 'train_loss': [], 'val_loss': [],\n",
    "        'train_acc': [], 'val_acc': [], 'learning_rate': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        if (unfreeze_every_x_epoch != 0) and (epoch % unfreeze_every_x_epoch == 0):\n",
    "            layers_to_unfreeze = epoch // unfreeze_every_x_epoch + 1\n",
    "            if not (layers_to_unfreeze > max_unfreeze_layers):    \n",
    "                print(f\"\\n[INFO] Unfreezing top {layers_to_unfreeze} BERT layers...\\n\")\n",
    "                unfreeze_bert_layers(model, layers_to_unfreeze, max_layers=max_unfreeze_layers)\n",
    "\n",
    "        # trainable = [name for name, param in model.named_parameters() if param.requires_grad]\n",
    "        # print(f\"[INFO] Trainable parameters after unfreezing: {len(trainable)}\")\n",
    "        # for name in trainable:\n",
    "        #     print(f\" - {name}\")\n",
    "\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for batch in tqdm(train_dataloader, desc=f'Epoch {epoch + 1}/{num_epochs}'):\n",
    "            embeddings = batch['embeddings'].to(device)\n",
    "            sequence = batch['sequence'].to(device)\n",
    "            labels = batch['label'].to(device)  # Changed from 'labels' to 'label'\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss, logits = model(embeddings, sequence, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            train_correct += (predictions == labels).sum().item()\n",
    "            train_total += labels.size(0)\n",
    "        \n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "        train_accuracy = train_correct / train_total\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_dataloader:\n",
    "                embeddings = batch['embeddings'].to(device)\n",
    "                sequence = batch['sequence'].to(device)\n",
    "                labels = batch['label'].to(device)  # Changed from 'labels' to 'label'\n",
    "                \n",
    "                loss, logits = model(embeddings, sequence, labels)\n",
    "                total_val_loss += loss.item()\n",
    "                \n",
    "                # Calculate accuracy\n",
    "                predictions = torch.argmax(logits, dim=1)\n",
    "                val_correct += (predictions == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "        \n",
    "        avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "        val_accuracy = val_correct / val_total\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step(avg_val_loss)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # Save history\n",
    "        history['epoch'].append(epoch + 1)\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['train_acc'].append(train_accuracy)\n",
    "        history['val_acc'].append(val_accuracy)\n",
    "        history['learning_rate'].append(current_lr)\n",
    "        \n",
    "        # Print metrics\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}:')\n",
    "        print(f'Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.4f}')\n",
    "        print(f'Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.4f}')\n",
    "        print(f'Learning Rate: {current_lr}')\n",
    "        \n",
    "        # Early stopping check\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        if patience_counter >= patience:\n",
    "            print(f'Early stopping triggered after epoch {epoch + 1}')\n",
    "            break\n",
    "    \n",
    "    # Load best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    # Convert history to DataFrame\n",
    "    history_df = pd.DataFrame(history)\n",
    "    return history_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Cell 6: Run the Pipeline\n",
    "# Initialize model and data\n",
    "model, train_dataloader, test_dataloader, device = prepare_model_and_data(\n",
    "    train_df=train_df,\n",
    "    test_df=test_df,\n",
    "    sample_size=10000\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history_df = train_model(\n",
    "    model=model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloader=test_dataloader,\n",
    "    device=device,\n",
    "    num_epochs=20,\n",
    "    patience=10,\n",
    "    learning_rate=1e-4,\n",
    "    unfreeze_every_x_epoch=2,\n",
    "    max_unfreeze_layers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Plot training history\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_df['train_loss'], label='Train Loss')\n",
    "plt.plot(history_df['val_loss'], label='Val Loss')\n",
    "plt.title('Loss Over Time')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_df['train_acc'], label='Train Acc')\n",
    "plt.plot(history_df['val_acc'], label='Val Acc')\n",
    "plt.title('Accuracy Over Time')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Cell 7: Compute Test Accuracy\n",
    "def compute_test_accuracy(model, test_dataloader, device):\n",
    "    model.eval()\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            embeddings = batch['embeddings'].to(device)\n",
    "            sequence = batch['sequence'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            _, logits = model(embeddings, sequence)\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            test_correct += (predictions == labels).sum().item()\n",
    "            test_total += labels.size(0)\n",
    "    \n",
    "    test_accuracy = test_correct / test_total\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Compute test accuracy\n",
    "compute_test_accuracy(model, test_dataloader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NERSC Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
