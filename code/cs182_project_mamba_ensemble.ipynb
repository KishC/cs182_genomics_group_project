{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip uninstall torch torchvision torchaudio transformers mamba-ssm tqdm pandas scikit-learn numpy -y\n",
    "# %pip cache purge\n",
    "# %pip install peft torch transformers mamba-ssm tqdm pandas scikit-learn numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>coord</th>\n",
       "      <th>kind</th>\n",
       "      <th>transcript</th>\n",
       "      <th>strand</th>\n",
       "      <th>chrom</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>sequence</th>\n",
       "      <th>win_start</th>\n",
       "      <th>win_end</th>\n",
       "      <th>is_truncated</th>\n",
       "      <th>motif_len</th>\n",
       "      <th>motif</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NC_050096.1_35318_donor</td>\n",
       "      <td>35318</td>\n",
       "      <td>0</td>\n",
       "      <td>XM_020544715.3</td>\n",
       "      <td>+</td>\n",
       "      <td>NC_050096.1</td>\n",
       "      <td>35319</td>\n",
       "      <td>35320</td>\n",
       "      <td>GGGCCCGGCTGGGCCTCAGCGGGGTCGTCGAGATGGAGATGGGGAG...</td>\n",
       "      <td>35118</td>\n",
       "      <td>35520</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>GT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NC_050096.1_34607_acceptor</td>\n",
       "      <td>34607</td>\n",
       "      <td>1</td>\n",
       "      <td>XM_020544715.3</td>\n",
       "      <td>+</td>\n",
       "      <td>NC_050096.1</td>\n",
       "      <td>34605</td>\n",
       "      <td>34606</td>\n",
       "      <td>TCCGGTGATTAATTTGTCCTTATACCTTTACAACAAAAATTCACTA...</td>\n",
       "      <td>34404</td>\n",
       "      <td>34806</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>TG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NC_050096.1_36174_donor</td>\n",
       "      <td>36174</td>\n",
       "      <td>0</td>\n",
       "      <td>XM_020544715.3</td>\n",
       "      <td>+</td>\n",
       "      <td>NC_050096.1</td>\n",
       "      <td>36175</td>\n",
       "      <td>36176</td>\n",
       "      <td>ATAATATGTTCATTATATCACAACACTCTTTTCTTATGGAGTCGTG...</td>\n",
       "      <td>35974</td>\n",
       "      <td>36376</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>GT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NC_050096.1_36037_acceptor</td>\n",
       "      <td>36037</td>\n",
       "      <td>1</td>\n",
       "      <td>XM_020544715.3</td>\n",
       "      <td>+</td>\n",
       "      <td>NC_050096.1</td>\n",
       "      <td>36035</td>\n",
       "      <td>36036</td>\n",
       "      <td>GCACAAAACTAACTAAAGGAATCATTCTGATAGATAACACTATAAA...</td>\n",
       "      <td>35834</td>\n",
       "      <td>36236</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>AG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NC_050096.1_36504_donor</td>\n",
       "      <td>36504</td>\n",
       "      <td>0</td>\n",
       "      <td>XM_020544715.3</td>\n",
       "      <td>+</td>\n",
       "      <td>NC_050096.1</td>\n",
       "      <td>36505</td>\n",
       "      <td>36506</td>\n",
       "      <td>TGTCATTTCCTTACCTCATTGAATCATTTCCGATGCTTCTTCTCTG...</td>\n",
       "      <td>36304</td>\n",
       "      <td>36706</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>GT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           id  coord  kind      transcript strand  \\\n",
       "0     NC_050096.1_35318_donor  35318     0  XM_020544715.3      +   \n",
       "1  NC_050096.1_34607_acceptor  34607     1  XM_020544715.3      +   \n",
       "2     NC_050096.1_36174_donor  36174     0  XM_020544715.3      +   \n",
       "3  NC_050096.1_36037_acceptor  36037     1  XM_020544715.3      +   \n",
       "4     NC_050096.1_36504_donor  36504     0  XM_020544715.3      +   \n",
       "\n",
       "         chrom  start    end  \\\n",
       "0  NC_050096.1  35319  35320   \n",
       "1  NC_050096.1  34605  34606   \n",
       "2  NC_050096.1  36175  36176   \n",
       "3  NC_050096.1  36035  36036   \n",
       "4  NC_050096.1  36505  36506   \n",
       "\n",
       "                                            sequence  win_start  win_end  \\\n",
       "0  GGGCCCGGCTGGGCCTCAGCGGGGTCGTCGAGATGGAGATGGGGAG...      35118    35520   \n",
       "1  TCCGGTGATTAATTTGTCCTTATACCTTTACAACAAAAATTCACTA...      34404    34806   \n",
       "2  ATAATATGTTCATTATATCACAACACTCTTTTCTTATGGAGTCGTG...      35974    36376   \n",
       "3  GCACAAAACTAACTAAAGGAATCATTCTGATAGATAACACTATAAA...      35834    36236   \n",
       "4  TGTCATTTCCTTACCTCATTGAATCATTTCCGATGCTTCTTCTCTG...      36304    36706   \n",
       "\n",
       "   is_truncated  motif_len motif  \n",
       "0         False          2    GT  \n",
       "1         False          2    TG  \n",
       "2         False          2    GT  \n",
       "3         False          2    AG  \n",
       "4         False          2    GT  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splice_df = pd.read_csv('../../splice_sites_full_centered_balanced_correct_V3.csv')\n",
    "splice_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data with stratification\n",
    "train_df, temp_df = train_test_split(splice_df, test_size=0.3, stratify=splice_df['kind'], random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['kind'], random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.4.0+cu121\n",
      "CUDA available: True\n",
      "CUDA version: 12.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from mamba_ssm import Mamba\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import os\n",
    "\n",
    "# Disable tokenizer parallelism to avoid warnings\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Dataset Class (Updated)\n",
    "class DNASequenceDataset(Dataset):\n",
    "    def __init__(self, sequences, labels, max_length=512):\n",
    "        self.sequences = sequences\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Nucleotide to index mapping (ACGT)\n",
    "        self.nuc_to_idx = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n",
    "        \n",
    "        # Preprocess sequences\n",
    "        self.seq_encodings = []\n",
    "        self.attention_masks = []\n",
    "        \n",
    "        for seq in sequences:\n",
    "            # Convert sequence to indices\n",
    "            encoding = [self.nuc_to_idx.get(nuc, 0) for nuc in seq[:max_length]]\n",
    "            encoding += [0] * (max_length - len(encoding))  # Pad with A\n",
    "            self.seq_encodings.append(torch.tensor(encoding, dtype=torch.long))\n",
    "            \n",
    "            # Create attention mask (1 for actual sequence, 0 for padding)\n",
    "            attention_mask = [1] * min(len(seq), max_length)\n",
    "            attention_mask += [0] * (max_length - len(attention_mask))\n",
    "            self.attention_masks.append(torch.tensor(attention_mask, dtype=torch.long))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'sequence': self.seq_encodings[idx],\n",
    "            'attention_mask': self.attention_masks[idx],\n",
    "            'label': self.labels[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Mamba Model (Updated)\n",
    "class MambaModel(nn.Module):\n",
    "    def __init__(self, d_model=128, d_state=16, d_conv=4, expand=2, num_mamba_layers=4, n_classes=3, max_length=512):\n",
    "        super().__init__()\n",
    "        self.nuc_embedding = nn.Embedding(6, d_model)\n",
    "        self.pos_embedding = nn.Embedding(max_length, d_model)\n",
    "        \n",
    "        nn.init.xavier_uniform_(self.nuc_embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.pos_embedding.weight)\n",
    "        \n",
    "        # Configure Mamba layers with explicit settings\n",
    "        self.mamba = nn.Sequential(*[\n",
    "            nn.Sequential(\n",
    "                nn.LayerNorm(d_model),\n",
    "                Mamba(\n",
    "                    d_model=d_model,\n",
    "                    d_state=d_state,\n",
    "                    d_conv=d_conv,\n",
    "                    expand=expand,\n",
    "                    use_fast_path=True,\n",
    "                    dt_rank=\"auto\",  # Add this to avoid potential issues\n",
    "                ),\n",
    "                nn.Dropout(0.1)\n",
    "            ) for _ in range(num_mamba_layers)\n",
    "        ])\n",
    "        \n",
    "        self.attention_pool = nn.Sequential(\n",
    "            nn.Linear(d_model, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(d_model, n_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, sequence):\n",
    "        pos_ids = torch.arange(sequence.size(1), device=sequence.device).unsqueeze(0).expand(sequence.size(0), -1)\n",
    "        x = self.nuc_embedding(sequence) + self.pos_embedding(pos_ids)\n",
    "        x = self.mamba(x)\n",
    "        weights = self.attention_pool(x)\n",
    "        x = (x * weights).sum(dim=1)\n",
    "        logits = self.classifier(x)\n",
    "        return logits\n",
    "    \n",
    "\n",
    "# Cell 4: DNABert2 Model (Simplified BERT)\n",
    "class DNABert2Model(nn.Module):\n",
    "    def __init__(self, n_classes=3, max_length=512):\n",
    "        super().__init__()\n",
    "        # Use standard BERT model instead of DNABERT-2\n",
    "        self.bert = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "        \n",
    "        # Add a projection layer to match DNA sequence vocabulary\n",
    "        self.dna_projection = nn.Linear(4, 768)  # 4 for ACGT\n",
    "        \n",
    "        # Freeze BERT parameters\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(768, n_classes)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Project DNA sequence embeddings to BERT dimension\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "        dna_embeddings = torch.zeros(batch_size, seq_len, 4, device=input_ids.device)\n",
    "        \n",
    "        # One-hot encode the DNA sequence\n",
    "        for i in range(4):  # ACGT\n",
    "            dna_embeddings[:, :, i] = (input_ids == i).float()\n",
    "            \n",
    "        # Project to BERT dimension\n",
    "        bert_input = self.dna_projection(dna_embeddings)\n",
    "        \n",
    "        # Get BERT outputs\n",
    "        outputs = self.bert(\n",
    "            inputs_embeds=bert_input,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "        \n",
    "        # Use the last hidden state's [CLS] token\n",
    "        pooled_output = outputs.last_hidden_state[:, 0, :]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Ensemble Model (Updated)\n",
    "class EnsembleModel(nn.Module):\n",
    "    def __init__(self, mamba_model, bert_model, n_classes=3):\n",
    "        super().__init__()\n",
    "        self.mamba_model = mamba_model\n",
    "        self.bert_model = bert_model\n",
    "        self.ensemble_weight = nn.Parameter(torch.tensor([0.5]))\n",
    "        \n",
    "    def forward(self, sequence, attention_mask, labels=None):\n",
    "        # Get predictions from both models\n",
    "        mamba_logits = self.mamba_model(sequence)\n",
    "        bert_logits = self.bert_model(sequence, attention_mask)\n",
    "        \n",
    "        # Combine predictions using learned weight\n",
    "        weight = torch.sigmoid(self.ensemble_weight)\n",
    "        combined_logits = weight * mamba_logits + (1 - weight) * bert_logits\n",
    "        \n",
    "        if labels is not None:\n",
    "            loss = nn.CrossEntropyLoss(label_smoothing=0.1)(combined_logits, labels)\n",
    "            return loss, combined_logits\n",
    "        return None, combined_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Model and Data Preparation (Updated)\n",
    "def prepare_model_and_data(train_df, test_df, sample_size=5000, max_length=512):\n",
    "    # Prepare datasets\n",
    "    train_sample = train_df.groupby('kind', group_keys=False).apply(\n",
    "        lambda x: x.sample(n=min(len(x), sample_size // 3))\n",
    "    ).reset_index(drop=True)\n",
    "    \n",
    "    test_sample = test_df.groupby('kind', group_keys=False).apply(\n",
    "        lambda x: x.sample(n=min(len(x), sample_size // 15))\n",
    "    ).reset_index(drop=True)\n",
    "    \n",
    "    train_dataset = DNASequenceDataset(\n",
    "        sequences=train_sample['sequence'].tolist(),\n",
    "        labels=train_sample['kind'].tolist(),\n",
    "        max_length=max_length\n",
    "    )\n",
    "    \n",
    "    test_dataset = DNASequenceDataset(\n",
    "        sequences=test_sample['sequence'].tolist(),\n",
    "        labels=test_sample['kind'].tolist(),\n",
    "        max_length=max_length\n",
    "    )\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=8)\n",
    "    \n",
    "    # Initialize models\n",
    "    mamba_model = MambaModel(\n",
    "        d_model=128,\n",
    "        d_state=16,\n",
    "        d_conv=4,\n",
    "        expand=1,\n",
    "        num_mamba_layers=4,\n",
    "        n_classes=3\n",
    "    ).to(device)\n",
    "    \n",
    "    bert_model = DNABert2Model(n_classes=3).to(device)\n",
    "    \n",
    "    # Create ensemble model\n",
    "    ensemble_model = EnsembleModel(mamba_model, bert_model).to(device)\n",
    "    \n",
    "    return ensemble_model, train_dataloader, test_dataloader, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Training Function (Updated)\n",
    "def train_model(model, train_dataloader, val_dataloader, device, \n",
    "                num_epochs=20, patience=3, learning_rate=1e-4):\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    history = {\n",
    "        'epoch': [], 'train_loss': [], 'val_loss': [],\n",
    "        'train_acc': [], 'val_acc': [], 'learning_rate': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for batch in tqdm(train_dataloader, desc=f'Epoch {epoch + 1}/{num_epochs}'):\n",
    "            sequence = batch['sequence'].to(device)  # Use 'sequence' instead of 'input_ids'\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss, logits = model(sequence, attention_mask, labels)  # Pass attention_mask instead of input_ids\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            train_correct += (predictions == labels).sum().item()\n",
    "            train_total += labels.size(0)\n",
    "        \n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "        train_accuracy = train_correct / train_total\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_dataloader:\n",
    "                sequence = batch['sequence'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['label'].to(device)\n",
    "                \n",
    "                loss, logits = model(sequence, attention_mask, labels)\n",
    "                total_val_loss += loss.item()\n",
    "                \n",
    "                predictions = torch.argmax(logits, dim=1)\n",
    "                val_correct += (predictions == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "        \n",
    "        avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "        val_accuracy = val_correct / val_total\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step(avg_val_loss)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # Print metrics\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}:\")\n",
    "        print(f\"Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.4f}\")\n",
    "        print(f\"Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
    "        print(f\"Learning Rate: {current_lr}\")\n",
    "        \n",
    "        # Update history\n",
    "        history['epoch'].append(epoch + 1)\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['train_acc'].append(train_accuracy)\n",
    "        history['val_acc'].append(val_accuracy)\n",
    "        history['learning_rate'].append(current_lr)\n",
    "        \n",
    "        # Early stopping\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model_state = model.state_dict()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping triggered after {epoch + 1} epochs\")\n",
    "                break\n",
    "    \n",
    "    # Load best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    # Convert history to DataFrame\n",
    "    history_df = pd.DataFrame(history)\n",
    "    return history_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1979078/2603563064.py:4: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  train_sample = train_df.groupby('kind', group_keys=False).apply(\n",
      "/tmp/ipykernel_1979078/2603563064.py:8: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  test_sample = test_df.groupby('kind', group_keys=False).apply(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69065748504c49b6b4cc465b85e036fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/20:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20:\n",
      "Train Loss: 1.0123, Train Acc: 0.4790\n",
      "Val Loss: 0.9082, Val Acc: 0.5976\n",
      "Learning Rate: 0.0001\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1e2a13f6df14119a2462d78ddfe29a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/20:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20:\n",
      "Train Loss: 0.7817, Train Acc: 0.7027\n",
      "Val Loss: 0.6396, Val Acc: 0.8048\n",
      "Learning Rate: 0.0001\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de7125b365294be79e1dc8bb401cc068",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/20:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20:\n",
      "Train Loss: 0.5924, Train Acc: 0.8385\n",
      "Val Loss: 0.5922, Val Acc: 0.8549\n",
      "Learning Rate: 0.0001\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "356a333b86164368b4bcb4fa45261efb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/20:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20:\n",
      "Train Loss: 0.5448, Train Acc: 0.8659\n",
      "Val Loss: 0.5792, Val Acc: 0.8549\n",
      "Learning Rate: 0.0001\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5f8552059f24b46aec6cc4c86e0c594",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/20:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20:\n",
      "Train Loss: 0.4953, Train Acc: 0.8962\n",
      "Val Loss: 0.6940, Val Acc: 0.7998\n",
      "Learning Rate: 0.0001\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d08330c0aa114cc8844b237f6915f87a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/20:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20:\n",
      "Train Loss: 0.4675, Train Acc: 0.9120\n",
      "Val Loss: 0.5755, Val Acc: 0.8559\n",
      "Learning Rate: 0.0001\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef789919fbae4103bdbcafee31acb3fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/20:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20:\n",
      "Train Loss: 0.4382, Train Acc: 0.9276\n",
      "Val Loss: 0.5889, Val Acc: 0.8639\n",
      "Learning Rate: 0.0001\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b317868b702a4b5cbae8eebaf7edab38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/20:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20:\n",
      "Train Loss: 0.4172, Train Acc: 0.9374\n",
      "Val Loss: 0.5846, Val Acc: 0.8619\n",
      "Learning Rate: 0.0001\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a23d174b3684f3aa4fbe57023db6786",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/20:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20:\n",
      "Train Loss: 0.3929, Train Acc: 0.9484\n",
      "Val Loss: 0.5784, Val Acc: 0.8659\n",
      "Learning Rate: 1e-05\n",
      "Early stopping triggered after 9 epochs\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Train the Ensemble Model\n",
    "model, train_dataloader, test_dataloader, device = prepare_model_and_data(train_df, test_df)\n",
    "history = train_model(model, train_dataloader, test_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.5253], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.ensemble_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
